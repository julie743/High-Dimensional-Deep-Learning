{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d50eb139-5964-4b5a-af0e-e4b69e1560aa",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "_Sentiment analysis through Recurrent Neural Networks_\n",
    "\n",
    "---\n",
    "\n",
    "In this tutorial, we are interested in the problem of sentiment analysis. In the first part, we will build a recurrent network on a toy dataset from scratch to determine if a sentence is positive or negative. In a second step, using the [`Keras`](https://keras.io/) API, we will build a network able to determine if a movie review is positive or negative.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ef5871b-799b-40e5-a2b0-863bfcf6e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b7b446-678c-45fe-80e4-a49c8448f3e2",
   "metadata": {},
   "source": [
    "---\n",
    "# PART I: RNN from Scratch\n",
    "\n",
    "In order to understand recurrent networks in more detail, our first example will be implementing a network from scratch. The network will perform a (simple) sentiment analysis task, namely determining whether a given text string is positive or negative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a36b5-ac50-4a26-9b14-e7a61017eac8",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "The commands below allow displaying some samples of our toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b73ff428-67f1-4538-89e2-e254c86e0051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', True),\n",
       " ('bad', False),\n",
       " ('happy', True),\n",
       " ('sad', False),\n",
       " ('not good', False),\n",
       " ('not bad', True),\n",
       " ('not happy', False),\n",
       " ('not sad', True),\n",
       " ('very good', True),\n",
       " ('very bad', False),\n",
       " ('very happy', True),\n",
       " ('very sad', False),\n",
       " ('i am happy', True),\n",
       " ('this is good', True),\n",
       " ('i am bad', False)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data import train_data, test_data\n",
    "\n",
    "list(train_data.items())[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7170f3e9-18b4-421d-a112-fb06cf127bf1",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "In order to visualize quickly the labels, we want display in _green_ the <span style=\"color:green\">positive sentences</span>, and in _red_ the <span style=\"color:orangered\">negative sentences</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f371d9b-de65-4f33-9a2c-2f482ccaa94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09441e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_data.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "709d7bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0323b412",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mFore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGREEN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "\n",
    "print(Fore.GREEN + list(train_data.item())[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c949fe-5a45-461c-99f7-8baba3fc7528",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Using the command `Fore.COLOR` of the package [`colorama`](https://pypi.org/project/colorama/), realize such a function.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97f973db-c41a-47b7-909e-7e3b7e880778",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "def coloredSentences(sentences, out=15):\n",
    "    \"\"\"\n",
    "    Display in green the positive sentences, and in red the negative sentences\n",
    "    - sentences is a dict\n",
    "        - sentences.keys() are the sentences to display\n",
    "        - sentences.values() are booleans that encode the sentiment\n",
    "    - out is an integer indicating the maximum number of sentences to display\n",
    "    \"\"\"\n",
    "    for txt in list(sentences.keys())[:out]:\n",
    "        if sentences[txt] == True:\n",
    "            print(Fore.GREEN + txt)\n",
    "        else:\n",
    "            print(Fore.RED + txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac11a88-34b0-4c1c-aa8b-9873947646e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scratch/coloredSentences.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "983c0401-f9c9-44ab-9ef7-5b4f7bce72fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mgood\n",
      "\u001b[31mbad\n",
      "\u001b[32mhappy\n",
      "\u001b[31msad\n",
      "\u001b[31mnot good\n",
      "\u001b[32mnot bad\n",
      "\u001b[31mnot happy\n",
      "\u001b[32mnot sad\n",
      "\u001b[32mvery good\n",
      "\u001b[31mvery bad\n",
      "\u001b[32mvery happy\n",
      "\u001b[31mvery sad\n",
      "\u001b[32mi am happy\n",
      "\u001b[32mthis is good\n",
      "\u001b[31mi am bad\n"
     ]
    }
   ],
   "source": [
    "coloredSentences(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45bcbe8-1af8-4528-8f3c-52851ed1be38",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "The datasets consists of two $\\texttt{dictionaries}$. Before trying to classify these sentences, we will build a vocabulary of all of all words that exist in our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43f59b2-9f2c-49cc-a18c-7226ab2b39d9",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Question:** How many different words are in our vocabulary?</span>\n",
    "\n",
    "To answer this question, start by building a **vocabulary**, _i.e._ a $\\texttt{list}$ containing all the words used in the dataset. _Each word should occur only once_.\n",
    "\n",
    "<!-- 18 unique words found -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93d8981f-29b0-46e7-9ba3-ab5018983886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 unique words found\n"
     ]
    }
   ],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "# il faut séparer chaque mot individuellement pour le vocabulaire\n",
    "vocab = list(set([w for text in train_data.keys() for w in text.split(' ')]))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print('%d unique words found' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93ea5e36-da35-4d9a-a6a7-789c9646cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scratch/vocab_size.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbee304-e47d-4726-bfa6-f09d81b400fd",
   "metadata": {},
   "source": [
    "### Word Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c1bc7-928c-44c9-8169-1b1b591ecd59",
   "metadata": {},
   "source": [
    "A neural network cannot take strings as input. So we have to encode these sentences in a format understandable by a computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3215ad08-6dd8-4f4e-9fbd-8534963c45f1",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Assign an integer index to represent each word of the vocab</span>\n",
    "\n",
    "To do that, construct two $\\texttt{dictionaries}$ allowing to translate words into integer indices, and vice versa :\n",
    "\n",
    "* $\\texttt{word_to_idx}$ has for keys the words of the vocabulary; and for value an integer index, the order in which the words appear in the vocabulary for example.\n",
    "* $\\texttt{idx_to_word}$ performs the opposite translation: its keys are the integer indices while its values are the associated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80d068ab-58fe-4c73-87e4-a8ff51610c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "happy\n"
     ]
    }
   ],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "idx = np.arange(0,vocab_size)\n",
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "for i in idx : \n",
    "    word_to_idx[vocab[i]]=i\n",
    "    idx_to_word[i] = vocab[i]\n",
    "\n",
    "print(word_to_idx['good'])\n",
    "print(idx_to_word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8955a2cb-3d3a-4dd4-b2ef-2a7e5962ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scratch/decode.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62a93da-4411-446f-bfce-93bf4f47fd51",
   "metadata": {},
   "source": [
    "This way of encoding words works quite well. However, it has the disadvantage of introducing a preferential but meaningless order in how words are processed. Since the vocabulary size is reasonable, we will use a one-hot encoding instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5709bea-e8eb-44e4-ad4a-ec4cdeeb3589",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Write a function $\\texttt{createInputs}$ that performs one-hot encoding</span>\n",
    "\n",
    "This function will return a $\\texttt{list}$ of the one-hot encodings of each word that compose the input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7f355dd2-3d90-447e-8452-fad744ddbe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "def createInputs2(text):\n",
    "    '''\n",
    "    Returns an array of one-hot vectors representing the words in the input text string.\n",
    "    - text is a string\n",
    "    - Each one-hot vector has shape (vocab_size, 1)\n",
    "    '''\n",
    "    # découpe le texte en mots individuels\n",
    "    text_words = [t for t in text.split(' ')]\n",
    "    text_size = len(text_words)\n",
    "    \n",
    "    oneHotMat = np.zeros((vocab_size,text_size))\n",
    "    \n",
    "    for col in range(text_size) :\n",
    "        ligne = word_to_idx[text_words[col]]\n",
    "        oneHotMat[ligne,col]=1\n",
    "    \n",
    "    return oneHotMat\n",
    "\n",
    "# on a fait une matrice, la structure qu'ils ont fait est une liste de vecteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ce782e44-3e95-4e8d-ad66-c1fc8d750c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scratch/createInputs.py\n",
    "def createInputs(text):\n",
    "    '''\n",
    "    Returns an array of one-hot vectors representing the words in the input text string.\n",
    "    - text is a string\n",
    "    - Each one-hot vector has shape (vocab_size, 1)\n",
    "    '''\n",
    "    inputs = []\n",
    "    for w in text.split(' '):\n",
    "        v = np.zeros((vocab_size, 1))\n",
    "        v[word_to_idx[w]] = 1\n",
    "        inputs.append(v)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c99eedd1-887e-485f-a948-db7b6db5d11d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createInputs('i am very good')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aa1677-ef5f-4137-9b0e-2ea557497232",
   "metadata": {},
   "source": [
    "## The Forward Phase\n",
    "\n",
    "In this part, we will build the simplest possible recursive network. To do so, we will create an $\\texttt{RNN}$ class that we will update as we build it. We want to classify a textual data. To do so, we will use a many-to-one network, as shown in the figure below.\n",
    "\n",
    "<img src=\"./img/many-to-one.png\" width=250>\n",
    "\n",
    "Let a sentence $x=(x_0,\\ldots,x_n)$, its label $y$, and let $h=(h_0,\\ldots,h_n)$ be the corresponding hidden state. We give ourselves three weight matrices, $W_{xh}$, $W_{hh}$ and $W_{hy}$, and two bias vectors, $b_h$ and $b_y$, so that, for any $t\\in[\\![0,n]\\!]$:\n",
    "\n",
    "$$ \\left\\{\\begin{aligned}\n",
    "    h_t &= \\tanh\\left( W_{xh}x_t + W_{hh}h_{t-1} + b_h \\right) \\\\\n",
    "    y &= W_{hy}h_n + b_y\n",
    "\\end{aligned}\\right. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498b536e-0edd-4b0f-9bcf-a3c9a8730cc6",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Question:** What is the dimension of the different weight matrices and bias vectors?</span>\n",
    "\n",
    "You can freely use the following notations:\n",
    "* $n_h$ denotes the $\\texttt{hidden_size}$, _i.e._ the size oh the hidden vectors $h_t$;\n",
    "* $n_x$ denotes the $\\texttt{input_size}$, _i.e._ the size of the inputs $x_t$;\n",
    "* $n_y$ denotes the $\\texttt{output_size}$, _i.e._ the size of the output $y$.\n",
    "\n",
    "Les tailles sont : \n",
    "* $Wxh = (n_h,n_x)$\n",
    "* $Whh = (n_h,n_h)$\n",
    "* $Why = (n_y,n_h)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81fb7e-25ad-409e-abe7-f69441a3b117",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d245a61a-05d5-4608-9a7b-d03bb71bec7c",
   "metadata": {},
   "source": [
    "<span style=\"color:teal \">[Solution]</span>\n",
    "\n",
    " **Solution**:\n",
    "* $W_{xh}\\in\\mathcal{M}_{n_h,n_x}(\\mathbb{R})$\n",
    "* $W_{hh}\\in\\mathcal{M}_{n_h,n_h}(\\mathbb{R})$\n",
    "* $W_{hy}\\in\\mathcal{M}_{n_y,n_h}(\\mathbb{R})$\n",
    "* $b_h\\in\\mathcal{M}_{n_h,1}(\\mathbb{R})$\n",
    "* $b_y\\in\\mathcal{M}_{n_y,1}(\\mathbb{R})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eca9c7-cca2-4147-85e9-a855758a7258",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Initialize the weight matrices and bias vectors. Realize the forward pass.</span>\n",
    "\n",
    "* The weights are initialized from the standard normal distribution, dividing by 1000 to reduce the initial variance. The biases are initialized to zero. \n",
    "* For the forward pass, first initialize the hidden state $h_0$ to zero, then perform each step of the RNN.\n",
    "\n",
    "**Note:** As said, dividing by 1000 the weights reduce the initial variance. This is not the best way to initialize weights, but it's simple and works for this simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73c0f0-57c8-457a-963e-f7b00f85cdbb",
   "metadata": {},
   "source": [
    "**Remark:** Before looking at the solution, you can test your $\\texttt{RNN}$ class by passing any input into the network. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7b462aae-40a4-4ecd-bb96-7cac866a8346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scratch/RNN_v1.py\n",
    "class RNN:\n",
    "    # A Vanilla Recurrent Neural Network.\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        # Weights\n",
    "        \n",
    "        # initialisation aléatoire des poids avec de petites valeurs. \n",
    "        # pour éviter une trop grande variance au début.\n",
    "        self.Whh = rd.randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wxh = rd.randn(hidden_size, input_size) / 1000\n",
    "        self.Why = rd.randn(output_size, hidden_size) / 1000\n",
    "\n",
    "        # Biases\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "    \n",
    "    # ----- #\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Perform a forward pass of the RNN using the given inputs.\n",
    "        Returns the final output and hidden state.\n",
    "        - inputs is a list of one-hot vectors with shape (input_size, 1).\n",
    "        '''\n",
    "        # inputs = (x_0,...,X_n) \n",
    "        \n",
    "        h = np.zeros((self.Whh.shape[0], 1)) #vecteur h_t de taille n_h*1\n",
    "\n",
    "        # Perform each step of the RNN\n",
    "        # on parcourt toutes les entrées (x_0,...,x_t,...,X_n) \n",
    "        # et on calcule le h_t associé à chaque x_t.\n",
    "        # Mais, on a pas besoin de stocker tous les h_t on  a juste besoin de h_n\n",
    "        # donc on réutilise le même vecteur h qu'on met à jour dans la boucle for \n",
    "        for x in inputs:\n",
    "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
    "\n",
    "        # Compute the output\n",
    "        # on calcule la sortie où h = h_n ici\n",
    "        y = self.Why @ h + self.by\n",
    "\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0134b449-9af4-4e0f-b69a-2f2109e85e63",
   "metadata": {},
   "source": [
    "The binary classification is performed using the $\\texttt{softmax}$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e382b7-2c91-400d-bae9-d5c4a8c5c21f",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Implement the softmax function.</span>\n",
    "\n",
    "As a reminder, for $x=(x_0,\\ldots,x_n)$ and $i_0\\in[\\![0,n]\\!]$, $~softmax(x_{i_0}) = \\frac{e^{x_{i_0}}}{\\sum_i e^{x_i}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d5559d7f-1ce8-4bf6-9b9f-1fe7493cd844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # Applies the Softmax function to the input array.\n",
    "    sum_S = np.sum(np.exp(x))\n",
    "    S = np.exp(x)/sum_S\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445066ad-64e0-4d8f-8c63-527eb373e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scratch/softmax.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b532e5f-8b5f-4729-ade4-fdfb935cb291",
   "metadata": {},
   "source": [
    "To ensure that we have not made an implementation error, we can pass a sentence from the training set through the network. Since the network has not yet been trained, we should find that this sentence is as likely to be positive as negative, i.e., a probability vector approximately equal to [0.5, 0.5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2596c28e-0591-480c-9560-74ef4cc1864b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49999759]\n",
      " [0.50000241]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RNN\n",
    "rnn = RNN(vocab_size, 2)\n",
    "\n",
    "inputs = createInputs('i am very good')\n",
    "out, _ = rnn.forward(inputs)\n",
    "\n",
    "probs = softmax(out)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5b96336e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4653e2b-af3b-434b-a655-dea00d664948",
   "metadata": {},
   "source": [
    "## The Backward Phase\n",
    "\n",
    "Lets move on to training. To this end, we first need a loss function. We will use the cross-entropy loss, which is often associated with the $softmax$ function. Let $\\sigma$ denotes the $softmax$ function and $y_c$ be the _correct_ class. Then:\n",
    "\n",
    "$$ \\mathcal{L} = \\mathcal{L}(x,y;W_{xh},W_{hh},W_{hy},b_h,b_y) = -\\log(p_c) \\qquad\\text{where}\\qquad p_c = \\sigma(y_c) \\,. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d002d-6db4-4662-b12f-d7dc8cf183a6",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Exercise:** Prove that for all $i\\in\\{0,1\\}$, $\\displaystyle\\quad\\frac{\\partial\\mathcal{L}}{\\partial y_i} = \\left\\{\\begin{aligned}\n",
    "    &p_i=\\sigma(y_i) & \\text{if}\\quad c\\neq i\\\\\n",
    "    &p_c-1=\\sigma(y_c)-1 & \\text{if}\\quad c=i\n",
    "\\end{aligned}\\right. $</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b733e7d-b7e9-4edd-ba36-718b24ac3c47",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d048a3-5b1a-4b87-bca1-490f2289ab49",
   "metadata": {},
   "source": [
    "<span style=\"color:teal \">[Solution]</span>\n",
    "\n",
    "**Solution**: $\\mathcal{L}(y_i)=-\\log(\\sigma(y_c))$. Hence, $\\displaystyle\\frac{\\partial\\mathcal{L}}{\\partial y_i}=-\\frac1{\\sigma(y_c)}\\times\\frac{\\partial\\sigma}{\\partial y_i}$.\n",
    "* If $i\\neq c$,\n",
    "$$ \\frac{\\partial\\sigma}{\\partial y_i} = \\frac{ -e^{y_c}\\times e^{y_i} }{ \\left(\\sum_ke^{y_k}\\right)^2 }\n",
    "    = \\frac{-e^{y_c}}{\\sum_ke^{y_k}}\\times\\frac{e^{y_i}}{\\sum_ke^{y_k}} = -\\sigma(y_c)\\times\\sigma(y_i) \n",
    "    \\qquad\\text{and}\\qquad\n",
    "   \\frac{\\partial\\mathcal{L}}{\\partial y_i} = -\\frac{-\\sigma(y_c)\\times\\sigma(y_i)}{\\sigma(y_c)} = \\sigma(y_i)=p_i \\,. $$\n",
    "\n",
    "* Else,\n",
    "$$ \\frac{\\partial\\sigma}{\\partial y_c} = \\frac{ e^{y_c}\\left(\\sum_ke^{y_k}\\right)-e^{y_c}\\times e^{y_c} }{ \\left(\\sum_ke^{y_k}\\right)^2 }\n",
    "    = \\frac{e^{y_c}}{\\sum_ke^{y_k}}-\\left(\\frac{e^{y_c}}{\\sum_ke^{y_k}}\\right)^2 = \\sigma(y_c)-\\sigma(y_c)^2 \n",
    "    \\qquad\\text{and}\\qquad\n",
    "   \\frac{\\partial\\mathcal{L}}{\\partial y_c} = -\\frac{\\sigma(y_c)-\\sigma(y_c)^2}{\\sigma(y_c)} = \\sigma(y_c)-1=p_c-1 \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaaeab7-29f1-4e11-9904-b443aea5a96a",
   "metadata": {},
   "source": [
    "Let us modify the $\\texttt{forward}$ function in the $\\texttt{RNN}$ class to cache (= enregistrer) the hidden states $h$ and the inputs $x$, which we will need for computing the gradients in the back-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "84b1fdde-aec5-45d9-a7ac-8e7c820e91ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    # A Vanilla Recurrent Neural Network.\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        # Weights\n",
    "        self.Whh = rd.randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wxh = rd.randn(hidden_size, input_size) / 1000\n",
    "        self.Why = rd.randn(output_size, hidden_size) / 1000\n",
    "\n",
    "        # Biases\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "    \n",
    "    # ----- #\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Perform a forward pass of the RNN using the given inputs.\n",
    "        Returns the final output and hidden state.\n",
    "        - inputs is an array of one-hot vectors with shape (input_size, 1).\n",
    "        '''\n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "\n",
    "        self.inputs = inputs  ### NEW ###\n",
    "        self.hs = { 0: h }    ### NEW ###\n",
    "        \n",
    "        # Perform each step of the RNN\n",
    "        for i, x in enumerate(inputs):\n",
    "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
    "            self.hs[i+1] = h  ### NEW ### on garde en mémoire tous les h_t\n",
    "            \n",
    "        # Compute the output\n",
    "        y = self.Why @ h + self.by\n",
    "\n",
    "        return y, h\n",
    "    \n",
    "    # ----- #\n",
    "    \n",
    "    def backprop(self, d_y, learn_rate=2e-2):\n",
    "        '''    \n",
    "        Perform a backward pass of the RNN.    \n",
    "        - d_y (dL/dy) has shape (output_size, 1).    \n",
    "        - learn_rate is a float.    \n",
    "        '''    \n",
    "        pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddc4fff-8b21-436a-88b9-ea81ab890754",
   "metadata": {},
   "source": [
    "Therefore, given a backward pass, we can train the RNN using the following loop on all training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0988a72c-7760-4efc-bfe4-844ba7253568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.49999599]\n",
      " [ 0.49999599]]\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/scratch/trainingLoop.py\n",
    "rnn = RNN(vocab_size, 2)\n",
    "\n",
    "# Loop over each training example\n",
    "for x, y in train_data.items():\n",
    "    inputs = createInputs(x)\n",
    "    target = int(y) # target : vraie classe (0 ou 1)\n",
    "\n",
    "    # Forward\n",
    "    out, _ = rnn.forward(inputs)\n",
    "    probs = softmax(out)\n",
    "\n",
    "    # Build dL/dy\n",
    "    d_L_d_y = probs \n",
    "    d_L_d_y[target] -= 1 # on enlève un sur la valeur ciblée (voir definition du dL/dy)\n",
    "    \n",
    "    # Backward\n",
    "    rnn.backprop(d_L_d_y) # pas encore active (pas d'entrainement du réseau ici), on fait une simple phase forward\n",
    "    \n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f23f52-7393-437a-baf6-db2878ec7564",
   "metadata": {},
   "source": [
    "### Gradient Computation\n",
    "\n",
    "It is then sufficient to backpropagate the gradient to train the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcff619-c716-4da1-a492-0836e2ac198c",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Question:** What are the parameters of the model to optimize?</span>\n",
    "\n",
    "Les matrices de poids et les vecteurs de biais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf18f38-e142-4b00-8598-8638eec82f5a",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c2f99-ef43-4a0d-beca-ebad89fc65c3",
   "metadata": {},
   "source": [
    "<span style=\"color:teal \">[Solution]</span>\n",
    "\n",
    "**Solution**: \n",
    "* The weights matrices $W_{xh}\\in\\mathcal{M}_{n_h,n_x}(\\mathbb{R})$, $W_{hh}\\in\\mathcal{M}_{n_h,n_h}(\\mathbb{R})$ and $W_{hy}\\in\\mathcal{M}_{n_y,n_h}(\\mathbb{R})$\n",
    "* The bias vectors $b_h\\in\\mathcal{M}_{n_h,1}(\\mathbb{R})$ and $b_y\\in\\mathcal{M}_{n_y,1}(\\mathbb{R})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf612ed-db24-47dc-b9b7-971b260416c0",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Exercise:** Compute the gradients $\\frac{\\partial\\mathcal{L}}{\\partial W_{hy}}$ and $\\frac{\\partial\\mathcal{L}}{\\partial b_y}$.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e593d8-3894-48f9-ad8f-192d7d692ff9",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3601a37f-1d4c-4a43-b3e9-84fc63a55526",
   "metadata": {},
   "source": [
    "<span style=\"color:teal \">[Solution]</span>\n",
    "\n",
    "**Solution**: Recall that $y=W_{hy}h_n+b_y$, where $h_n$ is the final hidden state. Then:\n",
    "* $\\displaystyle\\frac{\\partial\\mathcal{L}}{\\partial W_{hy}} \n",
    "    = \\frac{\\partial\\mathcal{L}}{\\partial y}\\times\\frac{\\partial y}{\\partial W_{hy}}\n",
    "    = \\frac{\\partial\\mathcal{L}}{\\partial y} h_n \\,;$\n",
    "    \n",
    "* $\\displaystyle\\frac{\\partial\\mathcal{L}}{\\partial b_y} \n",
    "    = \\frac{\\partial\\mathcal{L}}{\\partial y}\\times\\frac{\\partial y}{\\partial b_y}\n",
    "    = \\frac{\\partial\\mathcal{L}}{\\partial y} \\,.$\n",
    "    \n",
    "_Note:_ Beware of the dimensions of these objects! These are not partial derivatives in $\\mathbb{R}$...\n",
    "\n",
    "Quand on dérive par rapport à une matrice, c'est en fait une différentielle. Pour le définir il faudrait indiquer une direction de dérivation. Mais ce qu'on trouve en général comme notation c'est la dérivée partielle (on verra dans le code, que les calculs sont quand même différents selon les objets qu'on manipule). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94707e-9d88-4849-b782-d0f10f37edfd",
   "metadata": {},
   "source": [
    "Finally, we need the gradients for $W_{xh}$, $W_{hh}$, and $b_h$, which are used every step during the RNN. For example, for $W_{xh}$, we have \n",
    "$$ \\frac{\\partial\\mathcal{L}}{\\partial W_{xh}} = \\frac{\\partial\\mathcal{L}}{\\partial y} \\sum_{t=0}^n \\frac{\\partial y}{\\partial h_t}\\frac{\\partial h_t}{\\partial W_{xh}} $$\n",
    "because changing $W_{xh}$ affects every $h_t$, which all affect $y$ and ultimately $\\mathcal{L}$. In order to fully calculate the gradient of $W_{xh}$, we will need to backpropagate through all time-steps, which is known as Backpropagation Through Time (BPTT).\n",
    "\n",
    "<img src=\"img/bptt.png\" width=250>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae05a60-f044-4da2-8fa9-543b380d337c",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Exercise:** At a given time step $t$, compute $\\frac{\\partial h_t}{\\partial W_{xh}}$, $\\frac{\\partial h_t}{\\partial W_{hh}}$ and $\\frac{\\partial h_t}{\\partial b_h}$.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50eb5fa-99f7-443e-bfa5-3575478d2359",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd5b9a-c5d6-425b-93f8-fe3d889f172a",
   "metadata": {},
   "source": [
    "<span style=\"color:teal \">[Solution]</span>\n",
    "\n",
    " **Solution**: Recall that $h_t=\\tanh\\left( W_{xh}x_t + W_{hh}h_{t-1} + b_h \\right)$ and that $\\tanh^\\prime(x)=1-\\tanh^2(x)$. Then:\n",
    "\n",
    "* $\\displaystyle\\frac{\\partial h_t}{\\partial W_{xh}} = (1-h_t^2)\\,x_t \\,;$\n",
    "    \n",
    "* $\\displaystyle\\frac{\\partial h_t}{\\partial W_{hh}} = (1-h_t^2)\\,h_{t-1} \\,;$\n",
    "    \n",
    "* $\\displaystyle\\frac{\\partial h_t}{\\partial b_h} = (1-h_t^2) \\,.$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a41d3-a841-46a2-8d96-a3e7f2a38cc9",
   "metadata": {},
   "source": [
    "The last thing we need is $\\frac{\\partial y}{\\partial h_t}$. We can calculate it recursively:\n",
    "\n",
    "$$ \\forall t\\in[\\![0,n-1]\\!]\\,,\\quad  \\dfrac{\\partial y}{\\partial h_t} \n",
    "    = \\dfrac{\\partial y}{\\partial h_{t+1}}\\times\\dfrac{\\partial h_{t+1}}{\\partial h_t} \n",
    "    = \\dfrac{\\partial y}{\\partial h_{t+1}}\\,(1-h_t^2)\\,W_{hh}\n",
    "    \\qquad\\text{and}\\qquad \n",
    "    \\dfrac{\\partial y}{\\partial h_n}=W_{hy} \\,.$$\n",
    "    \n",
    "_Note:_ The recursion is _backward!_ We will implement BPTT starting from the last hidden state and working backwards, so we will already have $\\frac{\\partial y}{\\partial h_{t+1}}$ by the time we want to calculate $\\frac{\\partial y}{\\partial h_t}$.\n",
    "\n",
    "\n",
    "De plus, on a:\n",
    "\n",
    "$$ \\dfrac{\\partial L}{\\partial h_{n}} = \\dfrac{\\partial L}{\\partial y} \\times \\dfrac{\\partial y}{\\partial  h_{n}} \n",
    "= \\dfrac{\\partial L}{\\partial y} \\times W_{hy}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0611406-c339-481a-ae5b-53dbc0be775c",
   "metadata": {},
   "source": [
    "#### Back-Propagation Through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95254000-416e-452c-a205-322c781a80eb",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Using the previous gradients computations, implement the back-propagation through time.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0e8da0ca-8352-4e1e-8ec2-9913816a59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scratch/RNN_v2.py\n",
    "class RNN:\n",
    "    # A Vanilla Recurrent Neural Network.\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        # Weights\n",
    "        self.Whh = rd.randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wxh = rd.randn(hidden_size, input_size) / 1000\n",
    "        self.Why = rd.randn(output_size, hidden_size) / 1000\n",
    "\n",
    "        # Biases\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "    \n",
    "    # ----- #\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Perform a forward pass of the RNN using the given inputs.\n",
    "        Returns the final output and hidden state.\n",
    "        - inputs is an array of one-hot vectors with shape (input_size, 1).\n",
    "        '''\n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "\n",
    "        self.inputs = inputs\n",
    "        self.hs = { 0: h }\n",
    "        \n",
    "        # Perform each step of the RNN\n",
    "        for i, x in enumerate(inputs):\n",
    "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
    "            self.hs[i + 1] = h\n",
    "            \n",
    "        # Compute the output\n",
    "        y = self.Why @ h + self.by\n",
    "\n",
    "        return y, h\n",
    "    \n",
    "    # ----- #\n",
    "    \n",
    "    def backprop(self, d_y, learn_rate=2e-2):\n",
    "        '''    \n",
    "        Perform a backward pass of the RNN.    \n",
    "        - d_y (dL/dy) has shape (output_size, 1).    \n",
    "        - learn_rate is a float.    \n",
    "        '''    \n",
    "        n = len(self.inputs)# position du dernier h_n\n",
    "        # pour accéder aux h_t précédents, on fait \n",
    "        # self.hs[n-1], self.hs[n-2],...\n",
    "\n",
    "        # Calculate dL/dWhy and dL/dby.\n",
    "        d_Why = d_y @ self.hs[n].transpose() # on attrape le dernier h (h_n) avec self.hs[n]\n",
    "        d_by = d_y\n",
    "        \n",
    "        # Initialize dL/dWhh, dL/dWxh, and dL/dbh to zero.\n",
    "        d_Whh = np.zeros(self.Whh.shape)\n",
    "        d_Wxh = np.zeros(self.Wxh.shape)\n",
    "        d_bh = np.zeros(self.bh.shape)\n",
    "\n",
    "        # Calculate dL/dh for the last h.\n",
    "        d_h = self.Why.transpose() @ d_y\n",
    "\n",
    "        # Backpropagate through time.\n",
    "        for t in reversed(range(n)):\n",
    "            # An intermediate value: dL/dh * (1 - h^2)\n",
    "            tmp = (1 - self.hs[t+1]**2) * d_h\n",
    "\n",
    "            # dL/db = dL/dh * (1 - h^2)\n",
    "            d_bh += tmp \n",
    "            # dL/dWhh = dL/dh * (1 - h^2) * h_{t-1}\n",
    "            d_Whh += tmp @ self.hs[t].transpose()\n",
    "            # dL/dWxh = dL/dh * dh/dWxh = dL/dh * (1 - h^2) * x\n",
    "            d_Wxh += tmp @ self.inputs[t].transpose()\n",
    "            # Next dL/dh = dL/dh * (1 - h^2) * Whh\n",
    "            d_h = self.Whh @ tmp\n",
    "            \n",
    "        # Clip to prevent exploding gradients.\n",
    "        for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "            \n",
    "        # Update weights and biases using gradient descent.\n",
    "        self.Whh -= learn_rate * d_Whh\n",
    "        self.Wxh -= learn_rate * d_Wxh\n",
    "        self.Why -= learn_rate * d_Why\n",
    "        self.bh -= learn_rate * d_bh\n",
    "        self.by -= learn_rate * d_by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a8215",
   "metadata": {},
   "source": [
    "On a: \n",
    "$$ temp = \\dfrac{\\partial L}{\\partial h_{t}} \\times (1 - h_t^2) \n",
    "= \\dfrac{\\partial h_{t}}{\\partial b_{h}} \\times \\dfrac{\\partial b_{h}}{\\partial h_{t}} \\times (1 - h_t^2)\n",
    "= \\dfrac{\\partial h_{t}}{\\partial b_{h}} \\times \\dfrac{1}{(1-h_t^2)} \\times (1 - h_t^2)\n",
    "= \\dfrac{\\partial h_{t}}{\\partial b_{h}} \n",
    "$$\n",
    "\n",
    "$$ \\dfrac{\\partial L}{\\partial W_{hh}} = \\dfrac{\\partial L}{\\partial h_t} \\times \\dfrac{\\partial h_t}{\\partial W_{hh}}\n",
    "= \\dfrac{\\partial L}{\\partial h_t}(1-h_t^2)h_{t-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bedbd00-7275-4403-990f-6fa1a2a55149",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91edd547-6da6-4f91-a222-f889a1140af2",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Write a helper function to process data with the RNN.</span>\n",
    "\n",
    "To do this, you can refer to the various tests we have carried out previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "33acc766-bb03-4d82-8d32-037621d9de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scratch/processData.py\n",
    "def processData(data, backprop=True):\n",
    "    \n",
    "    '''\n",
    "    Returns the RNN's loss and accuracy for the given data.\n",
    "    - data is a dictionary mapping text to True or False.\n",
    "    - backprop determines if the backward phase should be run.\n",
    "    '''\n",
    "    \n",
    "    items = list(data.items())\n",
    "    random.shuffle(items)\n",
    "\n",
    "    loss = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    for x, y in items:\n",
    "        inputs = createInputs(x)\n",
    "        target = int(y)\n",
    "\n",
    "        # Forward\n",
    "        out, _ = rnn.forward(inputs)\n",
    "        probs = softmax(out) # prédiction\n",
    "\n",
    "        # Calculate loss / accuracy\n",
    "        loss -= np.log(probs[target])\n",
    "        num_correct += int(np.argmax(probs) == target) # on vérifie si la classe la plus probable = la target\n",
    "\n",
    "        if backprop:\n",
    "            # Build dL/dy\n",
    "            # direction de dérivation\n",
    "            d_L_d_y = probs\n",
    "            d_L_d_y[target] -= 1\n",
    "\n",
    "            # Backward : backpropagation dans la direction de dérivation\n",
    "            rnn.backprop(d_L_d_y)\n",
    "\n",
    "    return loss/len(data), num_correct/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ce1e1-ea95-4543-84f1-17b798e5cc4b",
   "metadata": {},
   "source": [
    "Last, we can write the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "780f3764-56bc-4005-9772-34f25e1ddace",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 100\n",
      "Train:\tLoss 0.689 | Accuracy: 0.552\n",
      "Test:\tLoss 0.698 | Accuracy: 0.500\n",
      "--- Epoch 200\n",
      "Train:\tLoss 0.671 | Accuracy: 0.621\n",
      "Test:\tLoss 0.716 | Accuracy: 0.550\n",
      "--- Epoch 300\n",
      "Train:\tLoss 0.529 | Accuracy: 0.690\n",
      "Test:\tLoss 0.632 | Accuracy: 0.700\n",
      "--- Epoch 400\n",
      "Train:\tLoss 0.013 | Accuracy: 1.000\n",
      "Test:\tLoss 0.013 | Accuracy: 1.000\n",
      "--- Epoch 500\n",
      "Train:\tLoss 0.004 | Accuracy: 1.000\n",
      "Test:\tLoss 0.004 | Accuracy: 1.000\n",
      "--- Epoch 600\n",
      "Train:\tLoss 0.002 | Accuracy: 1.000\n",
      "Test:\tLoss 0.003 | Accuracy: 1.000\n",
      "--- Epoch 700\n",
      "Train:\tLoss 0.002 | Accuracy: 1.000\n",
      "Test:\tLoss 0.002 | Accuracy: 1.000\n",
      "--- Epoch 800\n",
      "Train:\tLoss 0.251 | Accuracy: 0.914\n",
      "Test:\tLoss 0.407 | Accuracy: 0.800\n",
      "--- Epoch 900\n",
      "Train:\tLoss 0.003 | Accuracy: 1.000\n",
      "Test:\tLoss 0.004 | Accuracy: 1.000\n",
      "--- Epoch 1000\n",
      "Train:\tLoss 0.002 | Accuracy: 1.000\n",
      "Test:\tLoss 0.002 | Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(vocab_size, 2)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    train_loss, train_acc = processData(train_data)\n",
    "\n",
    "    if epoch % 100 == 99:\n",
    "        print('--- Epoch %d' % (epoch + 1))\n",
    "        print('Train:\\tLoss %.3f | Accuracy: %.3f' % (train_loss, train_acc))\n",
    "\n",
    "        test_loss, test_acc = processData(test_data, backprop=False)\n",
    "        print('Test:\\tLoss %.3f | Accuracy: %.3f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c5a81b-ad5d-492c-85eb-019037c79179",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Visualize the results of the training on the test data.</span>\n",
    "\n",
    "You will use the same color code as for the visualization of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c186ecee-9f92-4604-afc6-e558175c55e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "# Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "861f04e8-5a21-4d64-b3d6-f0beaffab2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mthis is happy\n",
      "\u001b[32mi am good\n",
      "\u001b[31mthis is not happy\n",
      "\u001b[31mi am not good\n",
      "\u001b[32mthis is not bad\n",
      "\u001b[32mi am not sad\n",
      "\u001b[32mi am very good\n",
      "\u001b[31mthis is very bad\n",
      "\u001b[31mi am very sad\n",
      "\u001b[31mthis is bad not good\n",
      "\u001b[32mthis is good and happy\n",
      "\u001b[31mi am not good and not happy\n",
      "\u001b[32mi am not at all sad\n",
      "\u001b[31mthis is not at all good\n",
      "\u001b[32mthis is not at all bad\n",
      "\u001b[32mthis is good right now\n",
      "\u001b[31mthis is sad right now\n",
      "\u001b[31mthis is very bad right now\n",
      "\u001b[32mthis was good earlier\n",
      "\u001b[31mi was not happy and not good earlier\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/scratch/coloredResults.py\n",
    "test_res = test_data\n",
    "\n",
    "for _, w in enumerate(test_res):\n",
    "    inputs = createInputs(w)\n",
    "    out, _ = rnn.forward(inputs)\n",
    "    res = softmax(out)<.5 # on teste les deux probas < 0.5\n",
    "    res = bool(res[0]) # on transforme la première classe en boolean (le 0). \n",
    "    # La classe 0 = sentiment négatif. La classe 1 = sentiment positif\n",
    "    test_res[w] = res\n",
    "    \n",
    "    if res: # teste si la proba de la classe sentiment négatif est <0.5\n",
    "        print(Fore.GREEN + w) # si c'est le cas c'est un sentiment positif (car classif binaire)\n",
    "    else: # sinon \n",
    "        print(Fore.RED + w) # c'est un sentiment négatif (proba la + élevée). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73994082-8f7d-4670-8228-81c138987c3f",
   "metadata": {},
   "source": [
    "# Part II: Study of the [IMDB](http://ai.stanford.edu/~amaas/data/sentiment/) Dataset\n",
    "\n",
    "<img src=\"./img/imdb.png\" width=500>\n",
    "\n",
    "In this second part, we will train a classifier movie reviews in IMDB data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7a5d5a77-5e48-411a-900d-02a3a099f29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1241d4-2b21-4681-8961-fe79735df19e",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d50de66b-11bb-489a-aaba-0199b2f8da66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 25000 training samples, 25000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(start_char=1, oov_char=2, index_from=3)\n",
    "\n",
    "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e757e60-06ff-4bd9-926a-762ec0cf6cbd",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "The commands below allow displaying a sample review and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "697f2089-488b-4ec7-9727-b1643a7887aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---review number---\n",
      "15836\n",
      "\n",
      "---review---\n",
      "[1, 13, 377, 86, 149, 5516, 54, 12, 385, 8, 248, 11, 4, 2285, 23, 80901, 54, 13, 16, 2001, 2437, 146, 150, 3333, 5, 131, 119, 12, 150, 17, 76, 17, 13, 122, 54, 13, 86, 296, 12, 13, 79, 6, 117, 1231, 39, 49, 7, 61, 369, 18, 131, 149, 6, 362, 123, 21, 13, 92, 459, 4002, 9682, 23216, 17, 15610, 9, 61, 965, 1640, 109, 5, 303, 23, 2021, 82, 877, 160, 7, 61, 1640, 105, 13, 377, 3427, 38, 76, 58, 149, 998, 318, 687, 43589, 5516, 23, 4, 248, 1709, 81270, 2285, 13, 119, 76750, 1820, 4479, 59, 16, 87, 11, 68602, 2568, 32, 21, 38, 76, 128, 11, 14, 13, 16, 16326, 54, 36, 872, 8, 1363, 12, 13, 440, 12, 80, 515, 30, 626, 23, 288, 133, 11, 4, 2285, 637, 30, 86, 11, 347, 4002, 1745]\n",
      "\n",
      "---label---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "idx = rd.randint(len(X_train))\n",
    "\n",
    "print('---review number---')\n",
    "print(idx) # numéro du film dont on fait la review\n",
    "\n",
    "print('\\n---review---')\n",
    "print(X_train[idx]) #review encodée par un vocobulaire  \n",
    "\n",
    "print('\\n---label---')\n",
    "print(y_train[idx]) # sentiment de la review "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324158e-1507-4166-92ef-17f9754f5861",
   "metadata": {},
   "source": [
    "The review is stored as a sequence of integers. These are word IDs that have been pre-assigned to individual words, based on their frequencies: the more frequent a word, the lower the integer. The label is an integer (0 for negative, 1 for positive).\n",
    "\n",
    "To decode the review, we need to use the vocabulary, _i.e._, the dictionary that associates each word with its unique integer ID, which is available via the `get_word_index()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d7ae1a01-aafe-424f-a258-cf317a3f2099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "pad_char = 0\n",
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 3\n",
    "\n",
    "word_to_idx = imdb.get_word_index()\n",
    "idx_to_word = {i+index_from: w for (w, i) in word_to_idx.items()}\n",
    "idx_to_word[pad_char] = \"[PAD]\"\n",
    "idx_to_word[start_char] = \"[START]\"\n",
    "idx_to_word[oov_char] = \"[OOV]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "671601d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{34704: 'fawn',\n",
       " 52009: 'tsukino',\n",
       " 52010: 'nunnery',\n",
       " 16819: 'sonja',\n",
       " 63954: 'vani',\n",
       " 1411: 'woods',\n",
       " 16118: 'spiders',\n",
       " 2348: 'hanging',\n",
       " 2292: 'woody',\n",
       " 52011: 'trawling',\n",
       " 52012: \"hold's\",\n",
       " 11310: 'comically',\n",
       " 40833: 'localized',\n",
       " 30571: 'disobeying',\n",
       " 52013: \"'royale\",\n",
       " 40834: \"harpo's\",\n",
       " 52014: 'canet',\n",
       " 19316: 'aileen',\n",
       " 52015: 'acurately',\n",
       " 52016: \"diplomat's\",\n",
       " 25245: 'rickman',\n",
       " 6749: 'arranged',\n",
       " 52017: 'rumbustious',\n",
       " 52018: 'familiarness',\n",
       " 52019: \"spider'\",\n",
       " 68807: 'hahahah',\n",
       " 52020: \"wood'\",\n",
       " 40836: 'transvestism',\n",
       " 34705: \"hangin'\",\n",
       " 2341: 'bringing',\n",
       " 40837: 'seamier',\n",
       " 34706: 'wooded',\n",
       " 52021: 'bravora',\n",
       " 16820: 'grueling',\n",
       " 1639: 'wooden',\n",
       " 16821: 'wednesday',\n",
       " 52022: \"'prix\",\n",
       " 34707: 'altagracia',\n",
       " 52023: 'circuitry',\n",
       " 11588: 'crotch',\n",
       " 57769: 'busybody',\n",
       " 52024: \"tart'n'tangy\",\n",
       " 14132: 'burgade',\n",
       " 52026: 'thrace',\n",
       " 11041: \"tom's\",\n",
       " 52028: 'snuggles',\n",
       " 29117: 'francesco',\n",
       " 52030: 'complainers',\n",
       " 52128: 'templarios',\n",
       " 40838: '272',\n",
       " 52031: '273',\n",
       " 52133: 'zaniacs',\n",
       " 34709: '275',\n",
       " 27634: 'consenting',\n",
       " 40839: 'snuggled',\n",
       " 15495: 'inanimate',\n",
       " 52033: 'uality',\n",
       " 11929: 'bronte',\n",
       " 4013: 'errors',\n",
       " 3233: 'dialogs',\n",
       " 52034: \"yomada's\",\n",
       " 34710: \"madman's\",\n",
       " 30588: 'dialoge',\n",
       " 52036: 'usenet',\n",
       " 40840: 'videodrome',\n",
       " 26341: \"kid'\",\n",
       " 52037: 'pawed',\n",
       " 30572: \"'girlfriend'\",\n",
       " 52038: \"'pleasure\",\n",
       " 52039: \"'reloaded'\",\n",
       " 40842: \"kazakos'\",\n",
       " 52040: 'rocque',\n",
       " 52041: 'mailings',\n",
       " 11930: 'brainwashed',\n",
       " 16822: 'mcanally',\n",
       " 52042: \"tom''\",\n",
       " 25246: 'kurupt',\n",
       " 21908: 'affiliated',\n",
       " 52043: 'babaganoosh',\n",
       " 40843: \"noe's\",\n",
       " 40844: 'quart',\n",
       " 362: 'kids',\n",
       " 5037: 'uplifting',\n",
       " 7096: 'controversy',\n",
       " 21909: 'kida',\n",
       " 23382: 'kidd',\n",
       " 52044: \"error'\",\n",
       " 52045: 'neurologist',\n",
       " 18513: 'spotty',\n",
       " 30573: 'cobblers',\n",
       " 9881: 'projection',\n",
       " 40845: 'fastforwarding',\n",
       " 52046: 'sters',\n",
       " 52047: \"eggar's\",\n",
       " 52048: 'etherything',\n",
       " 40846: 'gateshead',\n",
       " 34711: 'airball',\n",
       " 25247: 'unsinkable',\n",
       " 7183: 'stern',\n",
       " 52049: \"cervi's\",\n",
       " 40847: 'dnd',\n",
       " 11589: 'dna',\n",
       " 20601: 'insecurity',\n",
       " 52050: \"'reboot'\",\n",
       " 11040: 'trelkovsky',\n",
       " 52051: 'jaekel',\n",
       " 52052: 'sidebars',\n",
       " 52053: \"sforza's\",\n",
       " 17636: 'distortions',\n",
       " 52054: 'mutinies',\n",
       " 30605: 'sermons',\n",
       " 40849: '7ft',\n",
       " 52055: 'boobage',\n",
       " 52056: \"o'bannon's\",\n",
       " 23383: 'populations',\n",
       " 52057: 'chulak',\n",
       " 27636: 'mesmerize',\n",
       " 52058: 'quinnell',\n",
       " 10310: 'yahoo',\n",
       " 52060: 'meteorologist',\n",
       " 42580: 'beswick',\n",
       " 15496: 'boorman',\n",
       " 40850: 'voicework',\n",
       " 52061: \"ster'\",\n",
       " 22925: 'blustering',\n",
       " 52062: 'hj',\n",
       " 27637: 'intake',\n",
       " 5624: 'morally',\n",
       " 40852: 'jumbling',\n",
       " 52063: 'bowersock',\n",
       " 52064: \"'porky's'\",\n",
       " 16824: 'gershon',\n",
       " 40853: 'ludicrosity',\n",
       " 52065: 'coprophilia',\n",
       " 40854: 'expressively',\n",
       " 19503: \"india's\",\n",
       " 34713: \"post's\",\n",
       " 52066: 'wana',\n",
       " 5286: 'wang',\n",
       " 30574: 'wand',\n",
       " 25248: 'wane',\n",
       " 52324: 'edgeways',\n",
       " 34714: 'titanium',\n",
       " 40855: 'pinta',\n",
       " 181: 'want',\n",
       " 30575: 'pinto',\n",
       " 52068: 'whoopdedoodles',\n",
       " 21911: 'tchaikovsky',\n",
       " 2106: 'travel',\n",
       " 52069: \"'victory'\",\n",
       " 11931: 'copious',\n",
       " 22436: 'gouge',\n",
       " 52070: \"chapters'\",\n",
       " 6705: 'barbra',\n",
       " 30576: 'uselessness',\n",
       " 52071: \"wan'\",\n",
       " 27638: 'assimilated',\n",
       " 16119: 'petiot',\n",
       " 52072: 'most\\x85and',\n",
       " 3933: 'dinosaurs',\n",
       " 355: 'wrong',\n",
       " 52073: 'seda',\n",
       " 52074: 'stollen',\n",
       " 34715: 'sentencing',\n",
       " 40856: 'ouroboros',\n",
       " 40857: 'assimilates',\n",
       " 40858: 'colorfully',\n",
       " 27639: 'glenne',\n",
       " 52075: 'dongen',\n",
       " 4763: 'subplots',\n",
       " 52076: 'kiloton',\n",
       " 23384: 'chandon',\n",
       " 34716: \"effect'\",\n",
       " 27640: 'snugly',\n",
       " 40859: 'kuei',\n",
       " 9095: 'welcomed',\n",
       " 30074: 'dishonor',\n",
       " 52078: 'concurrence',\n",
       " 23385: 'stoicism',\n",
       " 14899: \"guys'\",\n",
       " 52080: \"beroemd'\",\n",
       " 6706: 'butcher',\n",
       " 40860: \"melfi's\",\n",
       " 30626: 'aargh',\n",
       " 20602: 'playhouse',\n",
       " 11311: 'wickedly',\n",
       " 1183: 'fit',\n",
       " 52081: 'labratory',\n",
       " 40862: 'lifeline',\n",
       " 1930: 'screaming',\n",
       " 4290: 'fix',\n",
       " 52082: 'cineliterate',\n",
       " 52083: 'fic',\n",
       " 52084: 'fia',\n",
       " 34717: 'fig',\n",
       " 52085: 'fmvs',\n",
       " 52086: 'fie',\n",
       " 52087: 'reentered',\n",
       " 30577: 'fin',\n",
       " 52088: 'doctresses',\n",
       " 52089: 'fil',\n",
       " 12609: 'zucker',\n",
       " 31934: 'ached',\n",
       " 52091: 'counsil',\n",
       " 52092: 'paterfamilias',\n",
       " 13888: 'songwriter',\n",
       " 34718: 'shivam',\n",
       " 9657: 'hurting',\n",
       " 302: 'effects',\n",
       " 52093: 'slauther',\n",
       " 52094: \"'flame'\",\n",
       " 52095: 'sommerset',\n",
       " 52096: 'interwhined',\n",
       " 27641: 'whacking',\n",
       " 52097: 'bartok',\n",
       " 8778: 'barton',\n",
       " 21912: 'frewer',\n",
       " 52098: \"fi'\",\n",
       " 6195: 'ingrid',\n",
       " 30578: 'stribor',\n",
       " 52099: 'approporiately',\n",
       " 52100: 'wobblyhand',\n",
       " 52101: 'tantalisingly',\n",
       " 52102: 'ankylosaurus',\n",
       " 17637: 'parasites',\n",
       " 52103: 'childen',\n",
       " 52104: \"jenkins'\",\n",
       " 52105: 'metafiction',\n",
       " 17638: 'golem',\n",
       " 40863: 'indiscretion',\n",
       " 23386: \"reeves'\",\n",
       " 57784: \"inamorata's\",\n",
       " 52107: 'brittannica',\n",
       " 7919: 'adapt',\n",
       " 30579: \"russo's\",\n",
       " 48249: 'guitarists',\n",
       " 10556: 'abbott',\n",
       " 40864: 'abbots',\n",
       " 17652: 'lanisha',\n",
       " 40866: 'magickal',\n",
       " 52108: 'mattter',\n",
       " 52109: \"'willy\",\n",
       " 34719: 'pumpkins',\n",
       " 52110: 'stuntpeople',\n",
       " 30580: 'estimate',\n",
       " 40867: 'ugghhh',\n",
       " 11312: 'gameplay',\n",
       " 52111: \"wern't\",\n",
       " 40868: \"n'sync\",\n",
       " 16120: 'sickeningly',\n",
       " 40869: 'chiara',\n",
       " 4014: 'disturbed',\n",
       " 40870: 'portmanteau',\n",
       " 52112: 'ineffectively',\n",
       " 82146: \"duchonvey's\",\n",
       " 37522: \"nasty'\",\n",
       " 1288: 'purpose',\n",
       " 52115: 'lazers',\n",
       " 28108: 'lightened',\n",
       " 52116: 'kaliganj',\n",
       " 52117: 'popularism',\n",
       " 18514: \"damme's\",\n",
       " 30581: 'stylistics',\n",
       " 52118: 'mindgaming',\n",
       " 46452: 'spoilerish',\n",
       " 52120: \"'corny'\",\n",
       " 34721: 'boerner',\n",
       " 6795: 'olds',\n",
       " 52121: 'bakelite',\n",
       " 27642: 'renovated',\n",
       " 27643: 'forrester',\n",
       " 52122: \"lumiere's\",\n",
       " 52027: 'gaskets',\n",
       " 887: 'needed',\n",
       " 34722: 'smight',\n",
       " 1300: 'master',\n",
       " 25908: \"edie's\",\n",
       " 40871: 'seeber',\n",
       " 52123: 'hiya',\n",
       " 52124: 'fuzziness',\n",
       " 14900: 'genesis',\n",
       " 12610: 'rewards',\n",
       " 30582: 'enthrall',\n",
       " 40872: \"'about\",\n",
       " 52125: \"recollection's\",\n",
       " 11042: 'mutilated',\n",
       " 52126: 'fatherlands',\n",
       " 52127: \"fischer's\",\n",
       " 5402: 'positively',\n",
       " 34708: '270',\n",
       " 34723: 'ahmed',\n",
       " 9839: 'zatoichi',\n",
       " 13889: 'bannister',\n",
       " 52130: 'anniversaries',\n",
       " 30583: \"helm's\",\n",
       " 52131: \"'work'\",\n",
       " 34724: 'exclaimed',\n",
       " 52132: \"'unfunny'\",\n",
       " 52032: '274',\n",
       " 547: 'feeling',\n",
       " 52134: \"wanda's\",\n",
       " 33269: 'dolan',\n",
       " 52136: '278',\n",
       " 52137: 'peacoat',\n",
       " 40873: 'brawny',\n",
       " 40874: 'mishra',\n",
       " 40875: 'worlders',\n",
       " 52138: 'protags',\n",
       " 52139: 'skullcap',\n",
       " 57599: 'dastagir',\n",
       " 5625: 'affairs',\n",
       " 7802: 'wholesome',\n",
       " 52140: 'hymen',\n",
       " 25249: 'paramedics',\n",
       " 52141: 'unpersons',\n",
       " 52142: 'heavyarms',\n",
       " 52143: 'affaire',\n",
       " 52144: 'coulisses',\n",
       " 40876: 'hymer',\n",
       " 52145: 'kremlin',\n",
       " 30584: 'shipments',\n",
       " 52146: 'pixilated',\n",
       " 30585: \"'00s\",\n",
       " 18515: 'diminishing',\n",
       " 1360: 'cinematic',\n",
       " 14901: 'resonates',\n",
       " 40877: 'simplify',\n",
       " 40878: \"nature'\",\n",
       " 40879: 'temptresses',\n",
       " 16825: 'reverence',\n",
       " 19505: 'resonated',\n",
       " 34725: 'dailey',\n",
       " 52147: '2\\x85',\n",
       " 27644: 'treize',\n",
       " 52148: 'majo',\n",
       " 21913: 'kiya',\n",
       " 52149: 'woolnough',\n",
       " 39800: 'thanatos',\n",
       " 35734: 'sandoval',\n",
       " 40882: 'dorama',\n",
       " 52150: \"o'shaughnessy\",\n",
       " 4991: 'tech',\n",
       " 32021: 'fugitives',\n",
       " 30586: 'teck',\n",
       " 76128: \"'e'\",\n",
       " 40884: 'doesn’t',\n",
       " 52152: 'purged',\n",
       " 660: 'saying',\n",
       " 41098: \"martians'\",\n",
       " 23421: 'norliss',\n",
       " 27645: 'dickey',\n",
       " 52155: 'dicker',\n",
       " 52156: \"'sependipity\",\n",
       " 8425: 'padded',\n",
       " 57795: 'ordell',\n",
       " 40885: \"sturges'\",\n",
       " 52157: 'independentcritics',\n",
       " 5748: 'tempted',\n",
       " 34727: \"atkinson's\",\n",
       " 25250: 'hounded',\n",
       " 52158: 'apace',\n",
       " 15497: 'clicked',\n",
       " 30587: \"'humor'\",\n",
       " 17180: \"martino's\",\n",
       " 52159: \"'supporting\",\n",
       " 52035: 'warmongering',\n",
       " 34728: \"zemeckis's\",\n",
       " 21914: 'lube',\n",
       " 52160: 'shocky',\n",
       " 7479: 'plate',\n",
       " 40886: 'plata',\n",
       " 40887: 'sturgess',\n",
       " 40888: \"nerds'\",\n",
       " 20603: 'plato',\n",
       " 34729: 'plath',\n",
       " 40889: 'platt',\n",
       " 52162: 'mcnab',\n",
       " 27646: 'clumsiness',\n",
       " 3902: 'altogether',\n",
       " 42587: 'massacring',\n",
       " 52163: 'bicenntinial',\n",
       " 40890: 'skaal',\n",
       " 14363: 'droning',\n",
       " 8779: 'lds',\n",
       " 21915: 'jaguar',\n",
       " 34730: \"cale's\",\n",
       " 1780: 'nicely',\n",
       " 4591: 'mummy',\n",
       " 18516: \"lot's\",\n",
       " 10089: 'patch',\n",
       " 50205: 'kerkhof',\n",
       " 52164: \"leader's\",\n",
       " 27647: \"'movie\",\n",
       " 52165: 'uncomfirmed',\n",
       " 40891: 'heirloom',\n",
       " 47363: 'wrangle',\n",
       " 52166: 'emotion\\x85',\n",
       " 52167: \"'stargate'\",\n",
       " 40892: 'pinoy',\n",
       " 40893: 'conchatta',\n",
       " 41131: 'broeke',\n",
       " 40894: 'advisedly',\n",
       " 17639: \"barker's\",\n",
       " 52169: 'descours',\n",
       " 775: 'lots',\n",
       " 9262: 'lotr',\n",
       " 9882: 'irs',\n",
       " 52170: 'lott',\n",
       " 40895: 'xvi',\n",
       " 34731: 'irk',\n",
       " 52171: 'irl',\n",
       " 6890: 'ira',\n",
       " 21916: 'belzer',\n",
       " 52172: 'irc',\n",
       " 27648: 'ire',\n",
       " 40896: 'requisites',\n",
       " 7696: 'discipline',\n",
       " 52964: 'lyoko',\n",
       " 11313: 'extend',\n",
       " 876: 'nature',\n",
       " 52173: \"'dickie'\",\n",
       " 40897: 'optimist',\n",
       " 30589: 'lapping',\n",
       " 3903: 'superficial',\n",
       " 52174: 'vestment',\n",
       " 2826: 'extent',\n",
       " 52175: 'tendons',\n",
       " 52176: \"heller's\",\n",
       " 52177: 'quagmires',\n",
       " 52178: 'miyako',\n",
       " 20604: 'moocow',\n",
       " 52179: \"coles'\",\n",
       " 40898: 'lookit',\n",
       " 52180: 'ravenously',\n",
       " 40899: 'levitating',\n",
       " 52181: 'perfunctorily',\n",
       " 30590: 'lookin',\n",
       " 40901: \"lot'\",\n",
       " 52182: 'lookie',\n",
       " 34873: 'fearlessly',\n",
       " 52184: 'libyan',\n",
       " 40902: 'fondles',\n",
       " 35717: 'gopher',\n",
       " 40904: 'wearying',\n",
       " 52185: \"nz's\",\n",
       " 27649: 'minuses',\n",
       " 52186: 'puposelessly',\n",
       " 52187: 'shandling',\n",
       " 31271: 'decapitates',\n",
       " 11932: 'humming',\n",
       " 40905: \"'nother\",\n",
       " 21917: 'smackdown',\n",
       " 30591: 'underdone',\n",
       " 40906: 'frf',\n",
       " 52188: 'triviality',\n",
       " 25251: 'fro',\n",
       " 8780: 'bothers',\n",
       " 52189: \"'kensington\",\n",
       " 76: 'much',\n",
       " 34733: 'muco',\n",
       " 22618: 'wiseguy',\n",
       " 27651: \"richie's\",\n",
       " 40907: 'tonino',\n",
       " 52190: 'unleavened',\n",
       " 11590: 'fry',\n",
       " 40908: \"'tv'\",\n",
       " 40909: 'toning',\n",
       " 14364: 'obese',\n",
       " 30592: 'sensationalized',\n",
       " 40910: 'spiv',\n",
       " 6262: 'spit',\n",
       " 7367: 'arkin',\n",
       " 21918: 'charleton',\n",
       " 16826: 'jeon',\n",
       " 21919: 'boardroom',\n",
       " 4992: 'doubts',\n",
       " 3087: 'spin',\n",
       " 53086: 'hepo',\n",
       " 27652: 'wildcat',\n",
       " 10587: 'venoms',\n",
       " 52194: 'misconstrues',\n",
       " 18517: 'mesmerising',\n",
       " 40911: 'misconstrued',\n",
       " 52195: 'rescinds',\n",
       " 52196: 'prostrate',\n",
       " 40912: 'majid',\n",
       " 16482: 'climbed',\n",
       " 34734: 'canoeing',\n",
       " 52198: 'majin',\n",
       " 57807: 'animie',\n",
       " 40913: 'sylke',\n",
       " 14902: 'conditioned',\n",
       " 40914: 'waddell',\n",
       " 52199: '3\\x85',\n",
       " 41191: 'hyperdrive',\n",
       " 34735: 'conditioner',\n",
       " 53156: 'bricklayer',\n",
       " 2579: 'hong',\n",
       " 52201: 'memoriam',\n",
       " 30595: 'inventively',\n",
       " 25252: \"levant's\",\n",
       " 20641: 'portobello',\n",
       " 52203: 'remand',\n",
       " 19507: 'mummified',\n",
       " 27653: 'honk',\n",
       " 19508: 'spews',\n",
       " 40915: 'visitations',\n",
       " 52204: 'mummifies',\n",
       " 25253: 'cavanaugh',\n",
       " 23388: 'zeon',\n",
       " 40916: \"jungle's\",\n",
       " 34736: 'viertel',\n",
       " 27654: 'frenchmen',\n",
       " 52205: 'torpedoes',\n",
       " 52206: 'schlessinger',\n",
       " 34737: 'torpedoed',\n",
       " 69879: 'blister',\n",
       " 52207: 'cinefest',\n",
       " 34738: 'furlough',\n",
       " 52208: 'mainsequence',\n",
       " 40917: 'mentors',\n",
       " 9097: 'academic',\n",
       " 20605: 'stillness',\n",
       " 40918: 'academia',\n",
       " 52209: 'lonelier',\n",
       " 52210: 'nibby',\n",
       " 52211: \"losers'\",\n",
       " 40919: 'cineastes',\n",
       " 4452: 'corporate',\n",
       " 40920: 'massaging',\n",
       " 30596: 'bellow',\n",
       " 19509: 'absurdities',\n",
       " 53244: 'expetations',\n",
       " 40921: 'nyfiken',\n",
       " 75641: 'mehras',\n",
       " 52212: 'lasse',\n",
       " 52213: 'visability',\n",
       " 33949: 'militarily',\n",
       " 52214: \"elder'\",\n",
       " 19026: 'gainsbourg',\n",
       " 20606: 'hah',\n",
       " 13423: 'hai',\n",
       " 34739: 'haj',\n",
       " 25254: 'hak',\n",
       " 4314: 'hal',\n",
       " 4895: 'ham',\n",
       " 53262: 'duffer',\n",
       " 52216: 'haa',\n",
       " 69: 'had',\n",
       " 11933: 'advancement',\n",
       " 16828: 'hag',\n",
       " 25255: \"hand'\",\n",
       " 13424: 'hay',\n",
       " 20607: 'mcnamara',\n",
       " 52217: \"mozart's\",\n",
       " 30734: 'duffel',\n",
       " 30597: 'haq',\n",
       " 13890: 'har',\n",
       " 47: 'has',\n",
       " 2404: 'hat',\n",
       " 40922: 'hav',\n",
       " 30598: 'haw',\n",
       " 52218: 'figtings',\n",
       " 15498: 'elders',\n",
       " 52219: 'underpanted',\n",
       " 52220: 'pninson',\n",
       " 27655: 'unequivocally',\n",
       " 23676: \"barbara's\",\n",
       " 52222: \"bello'\",\n",
       " 13000: 'indicative',\n",
       " 40923: 'yawnfest',\n",
       " 52223: 'hexploitation',\n",
       " 52224: \"loder's\",\n",
       " 27656: 'sleuthing',\n",
       " 32625: \"justin's\",\n",
       " 52225: \"'ball\",\n",
       " 52226: \"'summer\",\n",
       " 34938: \"'demons'\",\n",
       " 52228: \"mormon's\",\n",
       " 34740: \"laughton's\",\n",
       " 52229: 'debell',\n",
       " 39727: 'shipyard',\n",
       " 30600: 'unabashedly',\n",
       " 40404: 'disks',\n",
       " 2293: 'crowd',\n",
       " 10090: 'crowe',\n",
       " 56437: \"vancouver's\",\n",
       " 34741: 'mosques',\n",
       " 6630: 'crown',\n",
       " 52230: 'culpas',\n",
       " 27657: 'crows',\n",
       " 53347: 'surrell',\n",
       " 52232: 'flowless',\n",
       " 52233: 'sheirk',\n",
       " 40926: \"'three\",\n",
       " 52234: \"peterson'\",\n",
       " 52235: 'ooverall',\n",
       " 40927: 'perchance',\n",
       " 1324: 'bottom',\n",
       " 53366: 'chabert',\n",
       " 52236: 'sneha',\n",
       " 13891: 'inhuman',\n",
       " 52237: 'ichii',\n",
       " 52238: 'ursla',\n",
       " 30601: 'completly',\n",
       " 40928: 'moviedom',\n",
       " 52239: 'raddick',\n",
       " 51998: 'brundage',\n",
       " 40929: 'brigades',\n",
       " 1184: 'starring',\n",
       " 52240: \"'goal'\",\n",
       " 52241: 'caskets',\n",
       " 52242: 'willcock',\n",
       " 52243: \"threesome's\",\n",
       " 52244: \"mosque'\",\n",
       " 52245: \"cover's\",\n",
       " 17640: 'spaceships',\n",
       " 40930: 'anomalous',\n",
       " 27658: 'ptsd',\n",
       " 52246: 'shirdan',\n",
       " 21965: 'obscenity',\n",
       " 30602: 'lemmings',\n",
       " 30603: 'duccio',\n",
       " 52247: \"levene's\",\n",
       " 52248: \"'gorby'\",\n",
       " 25258: \"teenager's\",\n",
       " 5343: 'marshall',\n",
       " 9098: 'honeymoon',\n",
       " 3234: 'shoots',\n",
       " 12261: 'despised',\n",
       " 52249: 'okabasho',\n",
       " 8292: 'fabric',\n",
       " 18518: 'cannavale',\n",
       " 3540: 'raped',\n",
       " 52250: \"tutt's\",\n",
       " 17641: 'grasping',\n",
       " 18519: 'despises',\n",
       " 40931: \"thief's\",\n",
       " 8929: 'rapes',\n",
       " 52251: 'raper',\n",
       " 27659: \"eyre'\",\n",
       " 52252: 'walchek',\n",
       " 23389: \"elmo's\",\n",
       " 40932: 'perfumes',\n",
       " 21921: 'spurting',\n",
       " 52253: \"exposition'\\x85\",\n",
       " 52254: 'denoting',\n",
       " 34743: 'thesaurus',\n",
       " 40933: \"shoot'\",\n",
       " 49762: 'bonejack',\n",
       " 52256: 'simpsonian',\n",
       " 30604: 'hebetude',\n",
       " 34744: \"hallow's\",\n",
       " 52257: 'desperation\\x85',\n",
       " 34745: 'incinerator',\n",
       " 10311: 'congratulations',\n",
       " 52258: 'humbled',\n",
       " 5927: \"else's\",\n",
       " 40848: 'trelkovski',\n",
       " 52259: \"rape'\",\n",
       " 59389: \"'chapters'\",\n",
       " 52260: '1600s',\n",
       " 7256: 'martian',\n",
       " 25259: 'nicest',\n",
       " 52262: 'eyred',\n",
       " 9460: 'passenger',\n",
       " 6044: 'disgrace',\n",
       " 52263: 'moderne',\n",
       " 5123: 'barrymore',\n",
       " 52264: 'yankovich',\n",
       " 40934: 'moderns',\n",
       " 52265: 'studliest',\n",
       " 52266: 'bedsheet',\n",
       " 14903: 'decapitation',\n",
       " 52267: 'slurring',\n",
       " 52268: \"'nunsploitation'\",\n",
       " 34746: \"'character'\",\n",
       " 9883: 'cambodia',\n",
       " 52269: 'rebelious',\n",
       " 27660: 'pasadena',\n",
       " 40935: 'crowne',\n",
       " 52270: \"'bedchamber\",\n",
       " 52271: 'conjectural',\n",
       " 52272: 'appologize',\n",
       " 52273: 'halfassing',\n",
       " 57819: 'paycheque',\n",
       " 20609: 'palms',\n",
       " 52274: \"'islands\",\n",
       " 40936: 'hawked',\n",
       " 21922: 'palme',\n",
       " 40937: 'conservatively',\n",
       " 64010: 'larp',\n",
       " 5561: 'palma',\n",
       " 21923: 'smelling',\n",
       " 13001: 'aragorn',\n",
       " 52275: 'hawker',\n",
       " 52276: 'hawkes',\n",
       " 3978: 'explosions',\n",
       " 8062: 'loren',\n",
       " 52277: \"pyle's\",\n",
       " 6707: 'shootout',\n",
       " 18520: \"mike's\",\n",
       " 52278: \"driscoll's\",\n",
       " 40938: 'cogsworth',\n",
       " 52279: \"britian's\",\n",
       " 34747: 'childs',\n",
       " 52280: \"portrait's\",\n",
       " 3629: 'chain',\n",
       " 2500: 'whoever',\n",
       " 52281: 'puttered',\n",
       " 52282: 'childe',\n",
       " 52283: 'maywether',\n",
       " 3039: 'chair',\n",
       " 52284: \"rance's\",\n",
       " 34748: 'machu',\n",
       " 4520: 'ballet',\n",
       " 34749: 'grapples',\n",
       " 76155: 'summerize',\n",
       " 30606: 'freelance',\n",
       " 52286: \"andrea's\",\n",
       " 52287: '\\x91very',\n",
       " 45882: 'coolidge',\n",
       " 18521: 'mache',\n",
       " 52288: 'balled',\n",
       " 40940: 'grappled',\n",
       " 18522: 'macha',\n",
       " 21924: 'underlining',\n",
       " 5626: 'macho',\n",
       " 19510: 'oversight',\n",
       " 25260: 'machi',\n",
       " 11314: 'verbally',\n",
       " 21925: 'tenacious',\n",
       " 40941: 'windshields',\n",
       " 18560: 'paychecks',\n",
       " 3399: 'jerk',\n",
       " 11934: \"good'\",\n",
       " 34751: 'prancer',\n",
       " 21926: 'prances',\n",
       " 52289: 'olympus',\n",
       " 21927: 'lark',\n",
       " 10788: 'embark',\n",
       " 7368: 'gloomy',\n",
       " 52290: 'jehaan',\n",
       " 52291: 'turaqui',\n",
       " 20610: \"child'\",\n",
       " 2897: 'locked',\n",
       " 52292: 'pranced',\n",
       " 2591: 'exact',\n",
       " 52293: 'unattuned',\n",
       " 786: 'minute',\n",
       " 16121: 'skewed',\n",
       " 40943: 'hodgins',\n",
       " 34752: 'skewer',\n",
       " 52294: 'think\\x85',\n",
       " 38768: 'rosenstein',\n",
       " 52295: 'helmit',\n",
       " 34753: 'wrestlemanias',\n",
       " 16829: 'hindered',\n",
       " 30607: \"martha's\",\n",
       " 52296: 'cheree',\n",
       " 52297: \"pluckin'\",\n",
       " 40944: 'ogles',\n",
       " 11935: 'heavyweight',\n",
       " 82193: 'aada',\n",
       " 11315: 'chopping',\n",
       " 61537: 'strongboy',\n",
       " 41345: 'hegemonic',\n",
       " 40945: 'adorns',\n",
       " 41349: 'xxth',\n",
       " 34754: 'nobuhiro',\n",
       " 52301: 'capitães',\n",
       " 52302: 'kavogianni',\n",
       " 13425: 'antwerp',\n",
       " 6541: 'celebrated',\n",
       " 52303: 'roarke',\n",
       " 40946: 'baggins',\n",
       " 31273: 'cheeseburgers',\n",
       " 52304: 'matras',\n",
       " 52305: \"nineties'\",\n",
       " 52306: \"'craig'\",\n",
       " 13002: 'celebrates',\n",
       " 3386: 'unintentionally',\n",
       " 14365: 'drafted',\n",
       " 52307: 'climby',\n",
       " 52308: '303',\n",
       " 18523: 'oldies',\n",
       " 9099: 'climbs',\n",
       " 9658: 'honour',\n",
       " 34755: 'plucking',\n",
       " 30077: '305',\n",
       " 5517: 'address',\n",
       " 40947: 'menjou',\n",
       " 42595: \"'freak'\",\n",
       " 19511: 'dwindling',\n",
       " 9461: 'benson',\n",
       " 52310: 'white’s',\n",
       " 40948: 'shamelessness',\n",
       " 21928: 'impacted',\n",
       " 52311: 'upatz',\n",
       " 3843: 'cusack',\n",
       " 37570: \"flavia's\",\n",
       " 52312: 'effette',\n",
       " 34756: 'influx',\n",
       " 52313: 'boooooooo',\n",
       " 52314: 'dimitrova',\n",
       " 13426: 'houseman',\n",
       " 25262: 'bigas',\n",
       " 52315: 'boylen',\n",
       " 52316: 'phillipenes',\n",
       " 40949: 'fakery',\n",
       " 27661: \"grandpa's\",\n",
       " 27662: 'darnell',\n",
       " 19512: 'undergone',\n",
       " 52318: 'handbags',\n",
       " 21929: 'perished',\n",
       " 37781: 'pooped',\n",
       " 27663: 'vigour',\n",
       " 3630: 'opposed',\n",
       " 52319: 'etude',\n",
       " 11802: \"caine's\",\n",
       " 52320: 'doozers',\n",
       " 34757: 'photojournals',\n",
       " 52321: 'perishes',\n",
       " 34758: 'constrains',\n",
       " 40951: 'migenes',\n",
       " 30608: 'consoled',\n",
       " 16830: 'alastair',\n",
       " 52322: 'wvs',\n",
       " 52323: 'ooooooh',\n",
       " 34759: 'approving',\n",
       " 40952: 'consoles',\n",
       " 52067: 'disparagement',\n",
       " 52325: 'futureistic',\n",
       " 52326: 'rebounding',\n",
       " 52327: \"'date\",\n",
       " 52328: 'gregoire',\n",
       " 21930: 'rutherford',\n",
       " 34760: 'americanised',\n",
       " 82199: 'novikov',\n",
       " 1045: 'following',\n",
       " 34761: 'munroe',\n",
       " 52329: \"morita'\",\n",
       " 52330: 'christenssen',\n",
       " 23109: 'oatmeal',\n",
       " 25263: 'fossey',\n",
       " 40953: 'livered',\n",
       " 13003: 'listens',\n",
       " 76167: \"'marci\",\n",
       " 52333: \"otis's\",\n",
       " 23390: 'thanking',\n",
       " 16022: 'maude',\n",
       " 34762: 'extensions',\n",
       " 52335: 'ameteurish',\n",
       " 52336: \"commender's\",\n",
       " 27664: 'agricultural',\n",
       " 4521: 'convincingly',\n",
       " 17642: 'fueled',\n",
       " 54017: 'mahattan',\n",
       " 40955: \"paris's\",\n",
       " 52339: 'vulkan',\n",
       " 52340: 'stapes',\n",
       " 52341: 'odysessy',\n",
       " 12262: 'harmon',\n",
       " 4255: 'surfing',\n",
       " 23497: 'halloran',\n",
       " 49583: 'unbelieveably',\n",
       " 52342: \"'offed'\",\n",
       " 30610: 'quadrant',\n",
       " 19513: 'inhabiting',\n",
       " 34763: 'nebbish',\n",
       " 40956: 'forebears',\n",
       " 34764: 'skirmish',\n",
       " 52343: 'ocassionally',\n",
       " 52344: \"'resist\",\n",
       " 21931: 'impactful',\n",
       " 52345: 'spicier',\n",
       " 40957: 'touristy',\n",
       " 52346: \"'football'\",\n",
       " 40958: 'webpage',\n",
       " 52348: 'exurbia',\n",
       " 52349: 'jucier',\n",
       " 14904: 'professors',\n",
       " 34765: 'structuring',\n",
       " 30611: 'jig',\n",
       " 40959: 'overlord',\n",
       " 25264: 'disconnect',\n",
       " 82204: 'sniffle',\n",
       " 40960: 'slimeball',\n",
       " 40961: 'jia',\n",
       " 16831: 'milked',\n",
       " 40962: 'banjoes',\n",
       " 1240: 'jim',\n",
       " 52351: 'workforces',\n",
       " 52352: 'jip',\n",
       " 52353: 'rotweiller',\n",
       " 34766: 'mundaneness',\n",
       " 52354: \"'ninja'\",\n",
       " 11043: \"dead'\",\n",
       " 40963: \"cipriani's\",\n",
       " 20611: 'modestly',\n",
       " 52355: \"professor'\",\n",
       " 40964: 'shacked',\n",
       " 34767: 'bashful',\n",
       " 23391: 'sorter',\n",
       " 16123: 'overpowering',\n",
       " 18524: 'workmanlike',\n",
       " 27665: 'henpecked',\n",
       " 18525: 'sorted',\n",
       " 52357: \"jōb's\",\n",
       " 52358: \"'always\",\n",
       " 34768: \"'baptists\",\n",
       " 52359: 'dreamcatchers',\n",
       " 52360: \"'silence'\",\n",
       " 21932: 'hickory',\n",
       " 52361: 'fun\\x97yet',\n",
       " 52362: 'breakumentary',\n",
       " 15499: 'didn',\n",
       " 52363: 'didi',\n",
       " 52364: 'pealing',\n",
       " 40965: 'dispite',\n",
       " 25265: \"italy's\",\n",
       " 21933: 'instability',\n",
       " 6542: 'quarter',\n",
       " 12611: 'quartet',\n",
       " 52365: 'padmé',\n",
       " 52366: \"'bleedmedry\",\n",
       " 52367: 'pahalniuk',\n",
       " 52368: 'honduras',\n",
       " 10789: 'bursting',\n",
       " 41468: \"pablo's\",\n",
       " 52370: 'irremediably',\n",
       " 40966: 'presages',\n",
       " 57835: 'bowlegged',\n",
       " 65186: 'dalip',\n",
       " 6263: 'entering',\n",
       " 76175: 'newsradio',\n",
       " 54153: 'presaged',\n",
       " 27666: \"giallo's\",\n",
       " 40967: 'bouyant',\n",
       " 52371: 'amerterish',\n",
       " 18526: 'rajni',\n",
       " 30613: 'leeves',\n",
       " 34770: 'macauley',\n",
       " 615: 'seriously',\n",
       " 52372: 'sugercoma',\n",
       " 52373: 'grimstead',\n",
       " 52374: \"'fairy'\",\n",
       " 30614: 'zenda',\n",
       " 52375: \"'twins'\",\n",
       " 17643: 'realisation',\n",
       " 27667: 'highsmith',\n",
       " 7820: 'raunchy',\n",
       " 40968: 'incentives',\n",
       " 52377: 'flatson',\n",
       " 35100: 'snooker',\n",
       " 16832: 'crazies',\n",
       " 14905: 'crazier',\n",
       " 7097: 'grandma',\n",
       " 52378: 'napunsaktha',\n",
       " 30615: 'workmanship',\n",
       " 52379: 'reisner',\n",
       " 61309: \"sanford's\",\n",
       " 52380: '\\x91doña',\n",
       " 6111: 'modest',\n",
       " 19156: \"everything's\",\n",
       " 40969: 'hamer',\n",
       " 52382: \"couldn't'\",\n",
       " 13004: 'quibble',\n",
       " 52383: 'socking',\n",
       " 21934: 'tingler',\n",
       " 52384: 'gutman',\n",
       " 40970: 'lachlan',\n",
       " 52385: 'tableaus',\n",
       " 52386: 'headbanger',\n",
       " 2850: 'spoken',\n",
       " 34771: 'cerebrally',\n",
       " 23493: \"'road\",\n",
       " 21935: 'tableaux',\n",
       " 40971: \"proust's\",\n",
       " 40972: 'periodical',\n",
       " 52388: \"shoveller's\",\n",
       " 25266: 'tamara',\n",
       " 17644: 'affords',\n",
       " 3252: 'concert',\n",
       " 87958: \"yara's\",\n",
       " 52389: 'someome',\n",
       " 8427: 'lingering',\n",
       " 41514: \"abraham's\",\n",
       " 34772: 'beesley',\n",
       " 34773: 'cherbourg',\n",
       " 28627: 'kagan',\n",
       " 9100: 'snatch',\n",
       " 9263: \"miyazaki's\",\n",
       " 25267: 'absorbs',\n",
       " 40973: \"koltai's\",\n",
       " 64030: 'tingled',\n",
       " 19514: 'crossroads',\n",
       " 16124: 'rehab',\n",
       " 52392: 'falworth',\n",
       " 52393: 'sequals',\n",
       " ...}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c5f154-6256-4112-8462-d80be4a535cb",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Write a function that displays a review in a readable form along with its label.</span>\n",
    "\n",
    "Keep a similar display to the one suggested above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "78d2d771-5e3a-474e-a71c-d44c94080845",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "def decodeReview(idx):\n",
    "    '''\n",
    "    Converts the encoded idx-th review to human readable form.\n",
    "    Displays the review number, the review in words and the label\n",
    "    '''\n",
    "    text = []\n",
    "    review = X_train[idx]\n",
    "    for w in review:\n",
    "        text.append(idx_to_word[w])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea724d3a-805c-4106-bce7-173f6470539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/imdb/decodeReview.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "41dd2b39-656e-4a68-9886-d013c25f0d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[START]',\n",
       " 'i',\n",
       " 'remember',\n",
       " 'first',\n",
       " 'watching',\n",
       " 'sabrina',\n",
       " 'when',\n",
       " 'it',\n",
       " 'came',\n",
       " 'to',\n",
       " 'tv',\n",
       " 'in',\n",
       " 'the',\n",
       " 'uk',\n",
       " 'on',\n",
       " 'itv1',\n",
       " 'when',\n",
       " 'i',\n",
       " 'was',\n",
       " '13',\n",
       " '14',\n",
       " \"i'm\",\n",
       " 'now',\n",
       " '24',\n",
       " 'and',\n",
       " 'still',\n",
       " 'love',\n",
       " 'it',\n",
       " 'now',\n",
       " 'as',\n",
       " 'much',\n",
       " 'as',\n",
       " 'i',\n",
       " 'did',\n",
       " 'when',\n",
       " 'i',\n",
       " 'first',\n",
       " 'watched',\n",
       " 'it',\n",
       " 'i',\n",
       " 'get',\n",
       " 'a',\n",
       " 'little',\n",
       " 'stick',\n",
       " 'from',\n",
       " 'some',\n",
       " 'of',\n",
       " 'my',\n",
       " 'friends',\n",
       " 'for',\n",
       " 'still',\n",
       " 'watching',\n",
       " 'a',\n",
       " 'kids',\n",
       " 'show',\n",
       " 'but',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'care',\n",
       " 'lol',\n",
       " 'caroline',\n",
       " 'rhea',\n",
       " 'as',\n",
       " 'hilda',\n",
       " 'is',\n",
       " 'my',\n",
       " 'personal',\n",
       " 'favourite',\n",
       " 'character',\n",
       " 'and',\n",
       " 'later',\n",
       " 'on',\n",
       " 'morgan',\n",
       " 'also',\n",
       " 'became',\n",
       " 'another',\n",
       " 'of',\n",
       " 'my',\n",
       " 'favourite',\n",
       " 'characters',\n",
       " 'i',\n",
       " 'remember',\n",
       " 'spending',\n",
       " 'so',\n",
       " 'much',\n",
       " 'time',\n",
       " 'watching',\n",
       " 'various',\n",
       " 'special',\n",
       " 'events',\n",
       " 'honouring',\n",
       " 'sabrina',\n",
       " 'on',\n",
       " 'the',\n",
       " 'tv',\n",
       " 'station',\n",
       " 'nickeleoden',\n",
       " 'uk',\n",
       " 'i',\n",
       " 'love',\n",
       " 'mellissa',\n",
       " 'joan',\n",
       " 'hart',\n",
       " 'she',\n",
       " 'was',\n",
       " 'great',\n",
       " 'in',\n",
       " 'clarrissa',\n",
       " 'explains',\n",
       " 'all',\n",
       " 'but',\n",
       " 'so',\n",
       " 'much',\n",
       " 'better',\n",
       " 'in',\n",
       " 'this',\n",
       " 'i',\n",
       " 'was',\n",
       " 'gutted',\n",
       " 'when',\n",
       " 'they',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'finish',\n",
       " 'it',\n",
       " 'i',\n",
       " 'hope',\n",
       " 'it',\n",
       " 'will',\n",
       " 'soon',\n",
       " 'be',\n",
       " 'released',\n",
       " 'on',\n",
       " 'dvd',\n",
       " 'here',\n",
       " 'in',\n",
       " 'the',\n",
       " 'uk',\n",
       " \"i'll\",\n",
       " 'be',\n",
       " 'first',\n",
       " 'in',\n",
       " 'line',\n",
       " 'lol',\n",
       " 'x']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decodeReview(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754bb9e8-3dee-44fe-92a8-bc550315fc50",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Question:** What is the proportion of positive reviews in the training dataset? And in the test dataset?</span>\n",
    "\n",
    "This question can be answered using a barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "836342a5-87df-4ec8-986f-563a81a2eb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "# Proportion of positive reviews\n",
    "label, freq = np.unique(y_train, return_counts=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(x = label, height = freq)\n",
    "plt.xticks(label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3cb933fe-182e-472c-8450-2d70828b0d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train distribution:  {0: 12500, 1: 12500}\n",
      "y_test distribution:  {0: 12500, 1: 12500}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load solutions/imdb/positiveProportion.py\n",
    "plt.figure(figsize = (8,3))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.countplot(x=y_train)\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.title(\"y_train\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.countplot(x=y_test)\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.title(\"y_test\")\n",
    "\n",
    "\n",
    "unique,  counts = np.unique(y_train, return_counts = True)\n",
    "print(\"y_train distribution: \", dict(zip(unique,counts)))\n",
    "\n",
    "unique,  counts = np.unique(y_test, return_counts = True)\n",
    "print(\"y_test distribution: \", dict(zip(unique,counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3466221-afef-4569-b938-3a5ae2fc9f55",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Question:** How many different words does this database contain?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a4bf6811-438e-4642-9b31-6e49536b97c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88584 unique words found\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/imdb/vocab_size.py\n",
    "# on prend tout le vocabulaire existant. X_train et X_test ne sont que des fractions de ce jeu de données de vocab\n",
    "vocab_size = len(imdb.get_word_index()) \n",
    "print('%d unique words found' % vocab_size)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0cef1b-cdcf-45b0-86b8-210525b53f65",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Question:** Are all reviews the same length? If not, what is their maximum length?</span>\n",
    "\n",
    "This question can be answered using an histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ed8cac7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218,)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ccf5ccf3-b7aa-4b0a-8aa9-cd80eb2265fa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longueur maximale des reviews: 2494\n"
     ]
    }
   ],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "# Lengths of the reviews\n",
    "len_review = []\n",
    "for i in range(len(X_train)) : \n",
    "    len_review.append(len(X_train[i]))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(len_review)\n",
    "plt.xlabel(\"nombre de mots dans la review\")\n",
    "plt.ylabel(\"nombre de review\")\n",
    "plt.show()\n",
    "\n",
    "print(\"longueur maximale des reviews:\",np.max(len_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84721a47-77f6-472f-984a-f6e489c937f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/imdb/reviewsLengths.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5573faf-8645-44c4-bfb0-48f4b949bbcf",
   "metadata": {},
   "source": [
    "### Sequences Padding\n",
    "\n",
    "The reviews have a variable number of words, while the network has a fixed number of neurons. To get a fixed length input, we can simply truncate the reviews to a fixed number of words, say $\\texttt{max_words=200}$. To facilitate learning, we will also limit ourselves to the $\\texttt{vocab_size=10000}$ most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a604b375-3eb0-4785-a96a-076088893e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "60e1f3ad-6907-4a6b-9fb8-c64828665dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "max_words = 200\n",
    "vocab_size = 10000\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(start_char=1, oov_char=2, index_from=3, num_words = vocab_size)\n",
    "\n",
    "# X_train_pad = X_train où les review trop longues ont été tronquées à 200 mots \n",
    "# et où les reviews dont la taille est < 200 ont été étendues à une taille 200 \n",
    "# en rajoutant des 0 (valeur 0 pour que ces chiffres ne soient pas pris en compte dans l'apprentissage)\n",
    "X_train_pad = sequence.pad_sequences(X_train, value=0, padding='post', maxlen=max_words)\n",
    "X_test_pad = sequence.pad_sequences(X_test, value=0, padding='post', maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e95cf-98a2-439b-bd18-1c6ffe9bd09b",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Check that the size of the reviews is now equal to $\\texttt{max_words}$ for each of them.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6173e230-e8aa-488c-99cd-05b6cec5c873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "# Lengths of the reviews\n",
    "len_review = []\n",
    "for i in range(len(X_train_pad)) : \n",
    "    len_review.append(len(X_train_pad[i]))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(len_review)\n",
    "plt.xlabel(\"nombre de mots dans la review\")\n",
    "plt.ylabel(\"nombre de review\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82d19b7-f34d-40ec-9193-509423364404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/imdb/paddingLengths.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d855fcf-62ed-4ee9-8920-acadd29156b4",
   "metadata": {},
   "source": [
    "Let us see the effect of padding and truncation at the most frequent words on the previously displayed idx-th review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0c6d2e49-d0d9-4e71-806c-d17872c48c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[START]',\n",
       " 'i',\n",
       " 'remember',\n",
       " 'first',\n",
       " 'watching',\n",
       " 'sabrina',\n",
       " 'when',\n",
       " 'it',\n",
       " 'came',\n",
       " 'to',\n",
       " 'tv',\n",
       " 'in',\n",
       " 'the',\n",
       " 'uk',\n",
       " 'on',\n",
       " '[OOV]',\n",
       " 'when',\n",
       " 'i',\n",
       " 'was',\n",
       " '13',\n",
       " '14',\n",
       " \"i'm\",\n",
       " 'now',\n",
       " '24',\n",
       " 'and',\n",
       " 'still',\n",
       " 'love',\n",
       " 'it',\n",
       " 'now',\n",
       " 'as',\n",
       " 'much',\n",
       " 'as',\n",
       " 'i',\n",
       " 'did',\n",
       " 'when',\n",
       " 'i',\n",
       " 'first',\n",
       " 'watched',\n",
       " 'it',\n",
       " 'i',\n",
       " 'get',\n",
       " 'a',\n",
       " 'little',\n",
       " 'stick',\n",
       " 'from',\n",
       " 'some',\n",
       " 'of',\n",
       " 'my',\n",
       " 'friends',\n",
       " 'for',\n",
       " 'still',\n",
       " 'watching',\n",
       " 'a',\n",
       " 'kids',\n",
       " 'show',\n",
       " 'but',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'care',\n",
       " 'lol',\n",
       " 'caroline',\n",
       " '[OOV]',\n",
       " 'as',\n",
       " '[OOV]',\n",
       " 'is',\n",
       " 'my',\n",
       " 'personal',\n",
       " 'favourite',\n",
       " 'character',\n",
       " 'and',\n",
       " 'later',\n",
       " 'on',\n",
       " 'morgan',\n",
       " 'also',\n",
       " 'became',\n",
       " 'another',\n",
       " 'of',\n",
       " 'my',\n",
       " 'favourite',\n",
       " 'characters',\n",
       " 'i',\n",
       " 'remember',\n",
       " 'spending',\n",
       " 'so',\n",
       " 'much',\n",
       " 'time',\n",
       " 'watching',\n",
       " 'various',\n",
       " 'special',\n",
       " 'events',\n",
       " '[OOV]',\n",
       " 'sabrina',\n",
       " 'on',\n",
       " 'the',\n",
       " 'tv',\n",
       " 'station',\n",
       " '[OOV]',\n",
       " 'uk',\n",
       " 'i',\n",
       " 'love',\n",
       " '[OOV]',\n",
       " 'joan',\n",
       " 'hart',\n",
       " 'she',\n",
       " 'was',\n",
       " 'great',\n",
       " 'in',\n",
       " '[OOV]',\n",
       " 'explains',\n",
       " 'all',\n",
       " 'but',\n",
       " 'so',\n",
       " 'much',\n",
       " 'better',\n",
       " 'in',\n",
       " 'this',\n",
       " 'i',\n",
       " 'was',\n",
       " '[OOV]',\n",
       " 'when',\n",
       " 'they',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'finish',\n",
       " 'it',\n",
       " 'i',\n",
       " 'hope',\n",
       " 'it',\n",
       " 'will',\n",
       " 'soon',\n",
       " 'be',\n",
       " 'released',\n",
       " 'on',\n",
       " 'dvd',\n",
       " 'here',\n",
       " 'in',\n",
       " 'the',\n",
       " 'uk',\n",
       " \"i'll\",\n",
       " 'be',\n",
       " 'first',\n",
       " 'in',\n",
       " 'line',\n",
       " 'lol',\n",
       " 'x']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decodeReview(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d6fdc4-fee5-4070-80c8-0fe5b8e07399",
   "metadata": {},
   "source": [
    "## RNN for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "aecbc185-fa3c-48fd-b2d8-d9fdda80177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, SimpleRNN, LSTM, Dense, Dropout, Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6200be4-1088-4b0b-b37b-e512ef8ea133",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Design a RNN model for sentiment analysis.</span>\n",
    "\n",
    "The first layer must be an [`Embedding`](https://keras.io/api/layers/core_layers/embedding/) layer. To prevent gradient vanishing, choose a suitable recurrent network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9040f769-e436-46a9-8251-6ad95a8f5dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 32)           320000    \n",
      "=================================================================\n",
      "Total params: 320,000\n",
      "Trainable params: 320,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 11:53:57.631083: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-11-28 11:53:57.633870: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-28 11:53:57.637737: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "### TO BE COMPLETED ### \n",
    "embedding_size = 32\n",
    "\n",
    "rnn = Sequential(name=\"RNN\")\n",
    "rnn.add(Embedding(vocab_size, embedding_size, input_length=max_words))\n",
    "[...]\n",
    "\n",
    "print(rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0503022c-2e7a-4df3-80eb-7ed88a7f67e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic Tensor (lstm/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [180]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m rnn \u001b[38;5;241m=\u001b[39m Sequential(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRNN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m rnn\u001b[38;5;241m.\u001b[39madd(Embedding(vocab_size, embedding_size, input_length\u001b[38;5;241m=\u001b[39mmax_words))\n\u001b[0;32m----> 6\u001b[0m \u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m.5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43membedding_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m rnn\u001b[38;5;241m.\u001b[39madd(Dropout(\u001b[38;5;241m0.1\u001b[39m))\n\u001b[1;32m      8\u001b[0m rnn\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py:517\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 517\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py:223\u001b[0m, in \u001b[0;36mSequential.add\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_explicit_input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs:\n\u001b[1;32m    221\u001b[0m   \u001b[38;5;66;03m# If the model is being built continuously on top of an input layer:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m   \u001b[38;5;66;03m# refresh its output.\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m   output_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(nest\u001b[38;5;241m.\u001b[39mflatten(output_tensor)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(SINGLE_LAYER_OUTPUT_ERROR_MSG)\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py:660\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m inputs, initial_state, constants \u001b[38;5;241m=\u001b[39m _standardize_args(inputs,\n\u001b[1;32m    655\u001b[0m                                                      initial_state,\n\u001b[1;32m    656\u001b[0m                                                      constants,\n\u001b[1;32m    657\u001b[0m                                                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_constants)\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m constants \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 660\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;66;03m# If any of `initial_state` or `constants` are specified and are Keras\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;66;03m# tensors, then add them to the inputs and temporarily modify the\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;66;03m# input_spec to include them.\u001b[39;00m\n\u001b[1;32m    666\u001b[0m additional_inputs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py:951\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# Functional Model construction mode is invoked when `Layer`s are called on\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;66;03m# symbolic `KerasTensor`s, i.e.:\u001b[39;00m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;66;03m# >> inputs = tf.keras.Input(10)\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;66;03m# >> outputs = MyLayer()(inputs)  # Functional construction mode.\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;66;03m# >> model = tf.keras.Model(inputs, outputs)\u001b[39;00m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _in_functional_construction_mode(\u001b[38;5;28mself\u001b[39m, inputs, args, kwargs, input_list):\n\u001b[0;32m--> 951\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_functional_construction_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;66;03m# Maintains info about the `Layer.call` stack.\u001b[39;00m\n\u001b[1;32m    955\u001b[0m call_context \u001b[38;5;241m=\u001b[39m base_layer_utils\u001b[38;5;241m.\u001b[39mcall_context()\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py:1090\u001b[0m, in \u001b[0;36mLayer._functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keras_tensor\u001b[38;5;241m.\u001b[39mkeras_tensors_enabled():\n\u001b[1;32m   1087\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m call_context\u001b[38;5;241m.\u001b[39menter(\n\u001b[1;32m   1088\u001b[0m       layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, inputs\u001b[38;5;241m=\u001b[39minputs, build_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39mtraining_value):\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;66;03m# Check input assumptions set after layer building, e.g. input shape.\u001b[39;00m\n\u001b[0;32m-> 1090\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_keras_tensor_symbolic_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1094\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA layer\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms `call` method should return a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1095\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensor or a list of Tensors, not None \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1096\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(layer: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py:822\u001b[0m, in \u001b[0;36mLayer._keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mmap_structure(keras_tensor\u001b[38;5;241m.\u001b[39mKerasTensor, output_signature)\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 822\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_output_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_masks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py:863\u001b[0m, in \u001b[0;36mLayer._infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    857\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m    858\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Build layer if applicable (if the `build` method has been\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# overridden).\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;66;03m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[39;00m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_build(inputs)\n\u001b[0;32m--> 863\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_mask_metadata(inputs, outputs, input_masks,\n\u001b[1;32m    867\u001b[0m                         build_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent_v2.py:1157\u001b[0m, in \u001b[0;36mLSTM.call\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args_if_ragged(is_ragged_input, mask)\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# LSTM does not support constants. Ignore it during process.\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m inputs, initial_state, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   1160\u001b[0m   mask \u001b[38;5;241m=\u001b[39m mask[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py:859\u001b[0m, in \u001b[0;36mRNN._process_inputs\u001b[0;34m(self, inputs, initial_state, constants)\u001b[0m\n\u001b[1;32m    857\u001b[0m     initial_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m initial_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 859\u001b[0m   initial_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_initial_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(initial_state) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates):\n\u001b[1;32m    862\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer has \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates)) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    863\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m states but was passed \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(initial_state)) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    864\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m initial states.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py:642\u001b[0m, in \u001b[0;36mRNN.get_initial_state\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    640\u001b[0m dtype \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_initial_state_fn:\n\u001b[0;32m--> 642\u001b[0m   init_state \u001b[38;5;241m=\u001b[39m \u001b[43mget_initial_state_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    645\u001b[0m   init_state \u001b[38;5;241m=\u001b[39m _generate_zero_filled_state(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell\u001b[38;5;241m.\u001b[39mstate_size,\n\u001b[1;32m    646\u001b[0m                                            dtype)\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py:2506\u001b[0m, in \u001b[0;36mLSTMCell.get_initial_state\u001b[0;34m(self, inputs, batch_size, dtype)\u001b[0m\n\u001b[1;32m   2505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_initial_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 2506\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43m_generate_zero_filled_state_for_cell\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2507\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py:2987\u001b[0m, in \u001b[0;36m_generate_zero_filled_state_for_cell\u001b[0;34m(cell, inputs, batch_size, dtype)\u001b[0m\n\u001b[1;32m   2985\u001b[0m   batch_size \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mshape(inputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2986\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m-> 2987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_generate_zero_filled_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py:3003\u001b[0m, in \u001b[0;36m_generate_zero_filled_state\u001b[0;34m(batch_size_tensor, state_size, dtype)\u001b[0m\n\u001b[1;32m   3000\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m array_ops\u001b[38;5;241m.\u001b[39mzeros(init_state_size, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   3002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mis_nested(state_size):\n\u001b[0;32m-> 3003\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_zeros\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3004\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3005\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m create_zeros(state_size)\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/util/nest.py:659\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    656\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    660\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/util/nest.py:659\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    655\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    656\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    660\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py:3000\u001b[0m, in \u001b[0;36m_generate_zero_filled_state.<locals>.create_zeros\u001b[0;34m(unnested_state_size)\u001b[0m\n\u001b[1;32m   2998\u001b[0m flat_dims \u001b[38;5;241m=\u001b[39m tensor_shape\u001b[38;5;241m.\u001b[39mTensorShape(unnested_state_size)\u001b[38;5;241m.\u001b[39mas_list()\n\u001b[1;32m   2999\u001b[0m init_state_size \u001b[38;5;241m=\u001b[39m [batch_size_tensor] \u001b[38;5;241m+\u001b[39m flat_dims\n\u001b[0;32m-> 3000\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_state_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:201\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m    203\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:2819\u001b[0m, in \u001b[0;36m_tag_zeros_tensor.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2819\u001b[0m   tensor \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2820\u001b[0m   tensor\u001b[38;5;241m.\u001b[39m_is_zeros_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2821\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:2868\u001b[0m, in \u001b[0;36mzeros\u001b[0;34m(shape, dtype, name)\u001b[0m\n\u001b[1;32m   2864\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2865\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m   2866\u001b[0m     \u001b[38;5;66;03m# Create a constant if it won't be very big. Otherwise create a fill\u001b[39;00m\n\u001b[1;32m   2867\u001b[0m     \u001b[38;5;66;03m# op to prevent serialized GraphDefs from becoming too large.\u001b[39;00m\n\u001b[0;32m-> 2868\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43m_constant_if_small\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzero\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2870\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:2804\u001b[0m, in \u001b[0;36m_constant_if_small\u001b[0;34m(value, shape, dtype, name)\u001b[0m\n\u001b[1;32m   2802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_if_small\u001b[39m(value, shape, dtype, name):\n\u001b[1;32m   2803\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[1;32m   2805\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m constant(value, shape\u001b[38;5;241m=\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m   2806\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   2807\u001b[0m     \u001b[38;5;66;03m# Happens when shape is a Tensor, list with Tensor elements, etc.\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3051\u001b[0m, in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2933\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_prod_dispatcher)\n\u001b[1;32m   2934\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2935\u001b[0m          initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2936\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2937\u001b[0m \u001b[38;5;124;03m    Return the product of array elements over a given axis.\u001b[39;00m\n\u001b[1;32m   2938\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3049\u001b[0m \u001b[38;5;124;03m    10\u001b[39;00m\n\u001b[1;32m   3050\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprod\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3052\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:852\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 852\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    853\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert a symbolic Tensor (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) to a numpy array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    854\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m This error may indicate that you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre trying to pass a Tensor to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    855\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m a NumPy call, which is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (lstm/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported"
     ]
    }
   ],
   "source": [
    "# %load solutions/imdb/rnn.py\n",
    "embedding_size = 32\n",
    "\n",
    "rnn = Sequential(name=\"RNN\")\n",
    "rnn.add(Embedding(vocab_size, embedding_size, input_length=max_words))\n",
    "rnn.add(LSTM(int(.5*embedding_size)))\n",
    "rnn.add(Dropout(0.1))\n",
    "rnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(rnn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0790cd29-8efe-4323-8d8e-636b552825de",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Performing the learning.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba0d5e2-9a27-4726-bd1d-454f225cdb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 8\n",
    "\n",
    "X_valid, y_valid = X_train_pad[:batch_size], y_train_pad[:batch_size]\n",
    "X_train_rnn, y_train_rnn = X_train_pad[batch_size:], y_train_pad[batch_size:]\n",
    "\n",
    "\n",
    "rnn.compile(loss=..., \n",
    "             optimizer=..., \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history_rnn = rnn.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f99f0b1-e4bd-4ed7-acbf-6f3fec9be06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/imdb/rnnTraining.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a15083-b62d-4f38-b9f0-d20d61eab224",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Visualize the learning process.</span>\n",
    "\n",
    "Write a function that allows to represent on two different figures the accuracy on one hand, and the loss on the other hand, each for the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e2881-62e5-45f4-89b7-b37903fa9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "def plotTraining(history):\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c3829-0d53-4e1c-bc06-3308c4d4a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/imdb/plotTraining.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d4c15-7cbe-4c9c-882c-f343c8215dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTraining(history_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1237b8c5-61bf-48bc-9ee5-7da2c5fb0ed1",
   "metadata": {},
   "source": [
    "## Bidirectional RNN\n",
    "\n",
    "As defined, this network introduces a causal structure into the data. Also, for text processing, we often prefer a bidirectional network. To do this, we can use the [`Bidirectional`](https://keras.io/api/layers/recurrent_layers/bidirectional/) command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ee2e3c-588b-45f4-abff-f59217850130",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 32\n",
    "\n",
    "bi_rnn = Sequential(name=\"Bidirectional_RNN\")\n",
    "bi_rnn.add(Embedding(vocab_size, embedding_size, input_length=max_words))\n",
    "bi_rnn.add(Bidirectional(LSTM(int(.5*embedding_size))))  ### NEW ###\n",
    "bi_rnn.add(Dropout(0.1))\n",
    "bi_rnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(bi_rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a81c21-bed6-4f9d-b8e8-c2f1cb251a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 8\n",
    "\n",
    "X_valid, y_valid = X_train_pad[:batch_size], y_train[:batch_size]\n",
    "X_train_rnn, y_train_rnn = X_train_pad[batch_size:], y_train[batch_size:]\n",
    "\n",
    "\n",
    "bi_rnn.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history_bi_rnn = bi_rnn.fit(X_train_rnn, \n",
    "                    y_train_rnn, \n",
    "                    validation_data=(X_valid, y_valid), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=num_epochs)\n",
    "\n",
    "plotTraining(history_bi_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3023b1b0-0e6d-4d76-bf53-1abb53b34dad",
   "metadata": {},
   "source": [
    "Thanks to the $\\texttt{return_sequences}$ option, we can easily stack several RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b9883-309f-40b8-99c5-b1706115dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 32\n",
    "\n",
    "bi2_rnn = Sequential(name=\"Double_Bidirectional_RNN\")\n",
    "bi2_rnn.add(Embedding(vocab_size, embedding_size, input_length=max_words))\n",
    "bi2_rnn.add(Bidirectional(LSTM(int(.5*embedding_size), return_sequences = True)))\n",
    "bi2_rnn.add(Bidirectional(LSTM(int(.5*embedding_size), return_sequences = False)))\n",
    "bi2_rnn.add(Dropout(0.1))\n",
    "bi2_rnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(bi2_rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e7ef75-0bcf-4cce-b16f-962448b79d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 8\n",
    "\n",
    "X_valid, y_valid = X_train_pad[:batch_size], y_train[:batch_size]\n",
    "X_train_rnn, y_train_rnn = X_train_pad[batch_size:], y_train[batch_size:]\n",
    "\n",
    "\n",
    "bi2_rnn.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history_bi2_rnn = bi2_rnn.fit(X_train_rnn, \n",
    "                    y_train_rnn, \n",
    "                    validation_data=(X_valid, y_valid), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=num_epochs)\n",
    "\n",
    "plotTraining(history_bi2_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e1239e-fc01-4e3e-80f4-b90cf13449b6",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ffb9e-f111-421b-b807-2d9e7108662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d910c-b6f3-4190-bf04-64b7cd2bfdd0",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Compare the confusion matrices for the three models proposed above.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec9feee-09c1-4900-96a0-4af1f97d8250",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "# Compare the confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d60fa15-4e03-4779-939d-21543df46dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/imdb/confusion.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d183f7d-c21b-4417-ab91-102b43e604bd",
   "metadata": {},
   "source": [
    "## MLP for Sentiment Analysis\n",
    "\n",
    "Just to be sure of the usefulness of a recurrent network, we decide to test a \"simple\" perceptron on the IMDB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c556d-97ab-4d2d-8471-4feb3a050909",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Compare the above results with those of an MLP. Conclude.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38429f1f-d6d2-4111-8a23-4dc706e9c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ### \n",
    "\n",
    "# Comparison with a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7eb79-76ae-4a5a-89fe-39197ef46b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/imdb/mlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c2b16d-f333-49f6-b0b7-7daef40cba7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2541478-c6fa-4d0b-bbfb-3ddc821ca33e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
